# ğŸ¦ íˆ¬ìì ë™í–¥ MCP ì„œë²„ ìƒì„¸ ê°œë°œê³„íšì„œ

## ğŸ“‹ í”„ë¡œì íŠ¸ ê°œìš”

### ëª©í‘œ
í•œêµ­ ì£¼ì‹ì‹œì¥ì˜ íˆ¬ììë³„(ì™¸êµ­ì¸/ê¸°ê´€/ê°œì¸) ë§¤ë§¤ ë™í–¥ì„ ì‹¤ì‹œê°„ìœ¼ë¡œ ì¶”ì í•˜ê³  ë¶„ì„í•˜ëŠ” ê³ ì„±ëŠ¥ MCP ì„œë²„ êµ¬ì¶•

### í•µì‹¬ ê°€ì¹˜ ì œì•ˆ
- **ì‹¤ì‹œê°„ ë¶„ì„**: íˆ¬ììë³„ ë§¤ë§¤ ë™í–¥ ì‹¤ì‹œê°„ ëª¨ë‹ˆí„°ë§
- **ìŠ¤ë§ˆíŠ¸ë¨¸ë‹ˆ ì¶”ì **: ê³ ë„í™”ëœ ì•Œê³ ë¦¬ì¦˜ìœ¼ë¡œ ìŠ¤ë§ˆíŠ¸ë¨¸ë‹ˆ íë¦„ ê°ì§€
- **íŒ¨í„´ ì¸ì‹**: ML ê¸°ë°˜ íˆ¬ìì í–‰ë™ íŒ¨í„´ ë¶„ì„
- **ì•¡ì…˜ ì¸ì‚¬ì´íŠ¸**: ì‹¤í–‰ ê°€ëŠ¥í•œ íˆ¬ì ì‹œê·¸ë„ ì œê³µ

---

## ğŸ¯ Phase 1: ê¸°ë°˜ êµ¬ì¡° ì„¤ì • (4ì¼)

### Day 1: í”„ë¡œì íŠ¸ ì´ˆê¸°í™” ë° í™˜ê²½ ì„¤ì •

#### 1.1 ê°œë°œ í™˜ê²½ êµ¬ì„±
```bash
# í”„ë¡œì íŠ¸ êµ¬ì¡° ìƒì„±
mkdir -p src/{tools,api,analysis,utils}
mkdir -p tests/{unit,integration}
mkdir -p docs/{api,deployment}
mkdir -p config/{dev,prod}
```

#### 1.2 ì˜ì¡´ì„± ì„¤ì •
**requirements.txt í•µì‹¬ ë¼ì´ë¸ŒëŸ¬ë¦¬**
```
# MCP ê¸°ë³¸
mcp>=0.1.0

# ë¹„ë™ê¸° ì²˜ë¦¬
asyncio>=3.4.3
aiohttp>=3.8.0
uvloop>=0.17.0

# ë°ì´í„° ì²˜ë¦¬
pandas>=2.0.0
numpy>=1.24.0
pytz>=2023.3

# ì‹œê³„ì—´ ë¶„ì„
statsmodels>=0.14.0
scikit-learn>=1.3.0

# ë°ì´í„°ë² ì´ìŠ¤
asyncpg>=0.28.0
redis>=4.5.0

# API í´ë¼ì´ì–¸íŠ¸
requests>=2.31.0
websocket-client>=1.6.0

# ë¡œê¹… ë° ëª¨ë‹ˆí„°ë§
structlog>=23.1.0
prometheus-client>=0.17.0

# ë³´ì•ˆ
cryptography>=41.0.0
python-jose>=3.3.0
```

#### 1.3 í™˜ê²½ ë³€ìˆ˜ êµ¬ì„±
**.env.example**
```
# API ì„¤ì •
KOREA_INVESTMENT_APP_KEY=your_app_key
KOREA_INVESTMENT_APP_SECRET=your_app_secret
EBEST_APP_KEY=your_ebest_key
EBEST_APP_SECRET=your_ebest_secret

# ë°ì´í„°ë² ì´ìŠ¤
DATABASE_URL=postgresql://user:pass@localhost:5432/investor_trends
REDIS_URL=redis://localhost:6379/0

# ìºì‹± ì„¤ì •
CACHE_TTL_REALTIME=10
CACHE_TTL_MINUTE=60
CACHE_TTL_HOURLY=3600
CACHE_TTL_DAILY=86400

# ë¶„ì„ ì„¤ì •
SMART_MONEY_THRESHOLD=1000000000
LARGE_ORDER_THRESHOLD=500000000
ANOMALY_DETECTION_SENSITIVITY=2.5

# ë¡œê¹…
LOG_LEVEL=INFO
LOG_FORMAT=json

# ë³´ì•ˆ
SECRET_KEY=your_secret_key
TOKEN_EXPIRE_HOURS=24
```

### Day 2: ê¸°ë³¸ MCP ì„œë²„ êµ¬ì¡° êµ¬í˜„

#### 2.1 MCP ì„œë²„ ë©”ì¸ í´ë˜ìŠ¤
**src/server.py**
```python
import asyncio
from typing import Dict, List, Optional
from mcp.server import Server
from mcp.types import Tool, TextContent

from .tools.investor_tools import InvestorTools
from .tools.program_tools import ProgramTools
from .tools.ownership_tools import OwnershipTools
from .tools.analysis_tools import AnalysisTools
from .config import Config
from .utils.logger import setup_logger

class InvestorTrendsMCPServer:
    def __init__(self):
        self.server = Server("investor-trends")
        self.config = Config()
        self.logger = setup_logger()
        
        # ë„êµ¬ ì¸ìŠ¤í„´ìŠ¤ ì´ˆê¸°í™”
        self.investor_tools = InvestorTools(self.config)
        self.program_tools = ProgramTools(self.config)
        self.ownership_tools = OwnershipTools(self.config)
        self.analysis_tools = AnalysisTools(self.config)
        
        self._register_tools()
        self._setup_handlers()
    
    def _register_tools(self):
        """MCP ë„êµ¬ ë“±ë¡"""
        tools = [
            # íˆ¬ìì ë§¤ë§¤ ë™í–¥
            Tool(
                name="get_investor_trading",
                description="íˆ¬ììë³„ ë§¤ë§¤ ë™í–¥ ì¡°íšŒ",
                inputSchema={
                    "type": "object",
                    "properties": {
                        "stock_code": {"type": "string", "description": "ì¢…ëª© ì½”ë“œ"},
                        "investor_type": {
                            "type": "string", 
                            "enum": ["FOREIGN", "INSTITUTION", "INDIVIDUAL", "ALL"],
                            "default": "ALL"
                        },
                        "period": {
                            "type": "string",
                            "enum": ["1D", "5D", "20D", "60D"],
                            "default": "1D"
                        },
                        "market": {
                            "type": "string",
                            "enum": ["ALL", "KOSPI", "KOSDAQ"],
                            "default": "ALL"
                        }
                    }
                }
            ),
            # í”„ë¡œê·¸ë¨ ë§¤ë§¤
            Tool(
                name="get_program_trading",
                description="í”„ë¡œê·¸ë¨ ë§¤ë§¤ ë™í–¥ ì¡°íšŒ",
                inputSchema={
                    "type": "object",
                    "properties": {
                        "market": {
                            "type": "string",
                            "enum": ["KOSPI", "KOSDAQ", "ALL"],
                            "default": "ALL"
                        },
                        "program_type": {
                            "type": "string",
                            "enum": ["ALL", "ARBITRAGE", "NON_ARBITRAGE"],
                            "default": "ALL"
                        }
                    }
                }
            ),
            # ìŠ¤ë§ˆíŠ¸ë¨¸ë‹ˆ ì¶”ì 
            Tool(
                name="get_smart_money_tracker",
                description="ìŠ¤ë§ˆíŠ¸ë¨¸ë‹ˆ ì›€ì§ì„ ì¶”ì ",
                inputSchema={
                    "type": "object",
                    "properties": {
                        "detection_method": {
                            "type": "string",
                            "enum": ["LARGE_ORDERS", "INSTITUTION_CLUSTER", "FOREIGN_SURGE"],
                            "default": "LARGE_ORDERS"
                        },
                        "min_confidence": {
                            "type": "number",
                            "minimum": 0,
                            "maximum": 10,
                            "default": 7.0
                        }
                    }
                }
            )
        ]
        
        for tool in tools:
            self.server.register_tool(tool)
    
    async def run(self):
        """ì„œë²„ ì‹¤í–‰"""
        await self.server.run()

if __name__ == "__main__":
    server = InvestorTrendsMCPServer()
    asyncio.run(server.run())
```

#### 2.2 ì„¤ì • ê´€ë¦¬
**src/config.py**
```python
import os
from dataclasses import dataclass
from typing import Optional

@dataclass
class DatabaseConfig:
    url: str
    pool_size: int = 20
    max_overflow: int = 10
    pool_timeout: int = 30

@dataclass
class CacheConfig:
    redis_url: str
    ttl_realtime: int = 10
    ttl_minute: int = 60
    ttl_hourly: int = 3600
    ttl_daily: int = 86400

@dataclass
class APIConfig:
    korea_investment_key: str
    korea_investment_secret: str
    ebest_key: str
    ebest_secret: str
    rate_limit_per_minute: int = 200

@dataclass
class AnalysisConfig:
    smart_money_threshold: int = 1000000000
    large_order_threshold: int = 500000000
    anomaly_sensitivity: float = 2.5
    pattern_confidence_threshold: float = 0.7

class Config:
    def __init__(self):
        self.database = DatabaseConfig(
            url=os.getenv("DATABASE_URL", "postgresql://localhost/investor_trends")
        )
        
        self.cache = CacheConfig(
            redis_url=os.getenv("REDIS_URL", "redis://localhost:6379/0"),
            ttl_realtime=int(os.getenv("CACHE_TTL_REALTIME", "10")),
            ttl_minute=int(os.getenv("CACHE_TTL_MINUTE", "60")),
            ttl_hourly=int(os.getenv("CACHE_TTL_HOURLY", "3600")),
            ttl_daily=int(os.getenv("CACHE_TTL_DAILY", "86400"))
        )
        
        self.api = APIConfig(
            korea_investment_key=os.getenv("KOREA_INVESTMENT_APP_KEY", ""),
            korea_investment_secret=os.getenv("KOREA_INVESTMENT_APP_SECRET", ""),
            ebest_key=os.getenv("EBEST_APP_KEY", ""),
            ebest_secret=os.getenv("EBEST_APP_SECRET", "")
        )
        
        self.analysis = AnalysisConfig(
            smart_money_threshold=int(os.getenv("SMART_MONEY_THRESHOLD", "1000000000")),
            large_order_threshold=int(os.getenv("LARGE_ORDER_THRESHOLD", "500000000")),
            anomaly_sensitivity=float(os.getenv("ANOMALY_DETECTION_SENSITIVITY", "2.5"))
        )
```

### Day 3: API í´ë¼ì´ì–¸íŠ¸ ê¸°ë³¸ êµ¬í˜„

#### 3.1 í•œêµ­íˆ¬ìì¦ê¶Œ API í´ë¼ì´ì–¸íŠ¸
**src/api/korea_investment.py**
```python
import aiohttp
import asyncio
from typing import Dict, List, Optional
from datetime import datetime
import hashlib
import hmac
import base64

class KoreaInvestmentAPI:
    def __init__(self, app_key: str, app_secret: str):
        self.app_key = app_key
        self.app_secret = app_secret
        self.base_url = "https://openapi.koreainvestment.com:9443"
        self.access_token = None
        self.session = None
        
    async def __aenter__(self):
        self.session = aiohttp.ClientSession()
        await self._get_access_token()
        return self
        
    async def __aexit__(self, exc_type, exc_val, exc_tb):
        if self.session:
            await self.session.close()
    
    async def _get_access_token(self):
        """ì•¡ì„¸ìŠ¤ í† í° ë°œê¸‰"""
        url = f"{self.base_url}/oauth2/tokenP"
        
        data = {
            "grant_type": "client_credentials",
            "appkey": self.app_key,
            "appsecret": self.app_secret
        }
        
        async with self.session.post(url, json=data) as response:
            result = await response.json()
            self.access_token = result.get("access_token")
    
    async def get_investor_trading(
        self, 
        stock_code: Optional[str] = None,
        market: str = "ALL"
    ) -> Dict:
        """íˆ¬ììë³„ ë§¤ë§¤ ë™í–¥ ì¡°íšŒ"""
        
        headers = {
            "authorization": f"Bearer {self.access_token}",
            "appkey": self.app_key,
            "appsecret": self.app_secret,
            "tr_id": "FHKST130200000" if stock_code else "FHKST130100000"
        }
        
        params = {}
        if stock_code:
            params["fid_cond_mrkt_div_code"] = "J"
            params["fid_input_iscd"] = stock_code
        else:
            params["fid_cond_mrkt_div_code"] = market
            
        url = f"{self.base_url}/uapi/domestic-stock/v1/trading/investor-trading"
        
        async with self.session.get(url, headers=headers, params=params) as response:
            return await response.json()
    
    async def get_program_trading(self, market: str = "ALL") -> Dict:
        """í”„ë¡œê·¸ë¨ ë§¤ë§¤ ë™í–¥ ì¡°íšŒ"""
        
        headers = {
            "authorization": f"Bearer {self.access_token}",
            "appkey": self.app_key,
            "appsecret": self.app_secret,
            "tr_id": "FHKST130300000"
        }
        
        params = {
            "fid_cond_mrkt_div_code": market,
            "fid_input_date_1": datetime.now().strftime("%Y%m%d")
        }
        
        url = f"{self.base_url}/uapi/domestic-stock/v1/trading/program-trading"
        
        async with self.session.get(url, headers=headers, params=params) as response:
            return await response.json()
```

#### 3.2 ë°ì´í„° ëª¨ë¸ ì •ì˜
**src/api/models.py**
```python
from dataclasses import dataclass
from datetime import datetime
from typing import Optional, Dict, List
from decimal import Decimal

@dataclass
class InvestorData:
    """íˆ¬ìì ë°ì´í„° ëª¨ë¸"""
    buy_amount: int
    sell_amount: int
    net_amount: int
    buy_volume: int
    sell_volume: int
    net_volume: int
    average_buy_price: float
    average_sell_price: float
    net_ratio: float
    trend: str  # ACCUMULATING, DISTRIBUTING, NEUTRAL
    intensity: float  # 1-10

@dataclass
class StockInfo:
    """ì¢…ëª© ì •ë³´ ëª¨ë¸"""
    code: str
    name: str
    current_price: int
    change_rate: float
    market_cap: Optional[int] = None
    sector: Optional[str] = None

@dataclass
class InvestorTradingData:
    """íˆ¬ìì ë§¤ë§¤ ë°ì´í„° ì¢…í•© ëª¨ë¸"""
    timestamp: datetime
    scope: str  # MARKET, STOCK
    stock_info: Optional[StockInfo]
    foreign: InvestorData
    institution: InvestorData
    individual: InvestorData
    program: Dict
    market_impact: Dict
    
@dataclass
class SmartMoneySignal:
    """ìŠ¤ë§ˆíŠ¸ë¨¸ë‹ˆ ì‹ í˜¸ ëª¨ë¸"""
    stock_code: str
    stock_name: str
    signal_type: str
    confidence: float
    detection_details: Dict
    metrics: Dict
    technical_context: Dict
    timestamp: datetime

@dataclass
class ProgramTradingData:
    """í”„ë¡œê·¸ë¨ ë§¤ë§¤ ë°ì´í„° ëª¨ë¸"""
    timestamp: datetime
    market: str
    total_buy: int
    total_sell: int
    net_value: int
    arbitrage_data: Dict
    non_arbitrage_data: Dict
    market_indicators: Dict
```

### Day 4: ë°ì´í„°ë² ì´ìŠ¤ ì„¤ê³„ ë° ê¸°ë³¸ ì—°ê²°

#### 4.1 ë°ì´í„°ë² ì´ìŠ¤ ìŠ¤í‚¤ë§ˆ êµ¬í˜„
**database/schema.sql**
```sql
-- íˆ¬ìì ê±°ë˜ ë°ì´í„° í…Œì´ë¸”
CREATE TABLE investor_trading (
    id BIGSERIAL PRIMARY KEY,
    timestamp TIMESTAMPTZ NOT NULL,
    stock_code VARCHAR(10),
    market VARCHAR(10) NOT NULL,
    
    -- ì™¸êµ­ì¸ ë°ì´í„°
    foreign_buy BIGINT DEFAULT 0,
    foreign_sell BIGINT DEFAULT 0,
    foreign_net BIGINT DEFAULT 0,
    foreign_buy_volume BIGINT DEFAULT 0,
    foreign_sell_volume BIGINT DEFAULT 0,
    
    -- ê¸°ê´€ ë°ì´í„°
    institution_buy BIGINT DEFAULT 0,
    institution_sell BIGINT DEFAULT 0,
    institution_net BIGINT DEFAULT 0,
    institution_buy_volume BIGINT DEFAULT 0,
    institution_sell_volume BIGINT DEFAULT 0,
    
    -- ê°œì¸ ë°ì´í„°
    individual_buy BIGINT DEFAULT 0,
    individual_sell BIGINT DEFAULT 0,
    individual_net BIGINT DEFAULT 0,
    individual_buy_volume BIGINT DEFAULT 0,
    individual_sell_volume BIGINT DEFAULT 0,
    
    -- í”„ë¡œê·¸ë¨ ë°ì´í„°
    program_buy BIGINT DEFAULT 0,
    program_sell BIGINT DEFAULT 0,
    program_net BIGINT DEFAULT 0,
    
    -- ë©”íƒ€ë°ì´í„°
    created_at TIMESTAMPTZ DEFAULT NOW(),
    updated_at TIMESTAMPTZ DEFAULT NOW()
);

-- TimescaleDB í•˜ì´í¼í…Œì´ë¸” ìƒì„±
SELECT create_hypertable('investor_trading', 'timestamp', if_not_exists => TRUE);

-- ì¸ë±ìŠ¤ ìƒì„±
CREATE INDEX idx_investor_trading_stock_time ON investor_trading (stock_code, timestamp DESC);
CREATE INDEX idx_investor_trading_market_time ON investor_trading (market, timestamp DESC);
CREATE INDEX idx_investor_trading_foreign_net ON investor_trading (foreign_net);
CREATE INDEX idx_investor_trading_institution_net ON investor_trading (institution_net);

-- ê¸°ê´€ íˆ¬ìì ì„¸ë¶€ ë°ì´í„°
CREATE TABLE institution_detail (
    id BIGSERIAL PRIMARY KEY,
    timestamp TIMESTAMPTZ NOT NULL,
    stock_code VARCHAR(10),
    institution_type VARCHAR(50) NOT NULL,
    buy_amount BIGINT DEFAULT 0,
    sell_amount BIGINT DEFAULT 0,
    net_amount BIGINT DEFAULT 0,
    created_at TIMESTAMPTZ DEFAULT NOW()
);

SELECT create_hypertable('institution_detail', 'timestamp', if_not_exists => TRUE);

-- ìŠ¤ë§ˆíŠ¸ë¨¸ë‹ˆ ì‹ í˜¸ í…Œì´ë¸”
CREATE TABLE smart_money_signals (
    id BIGSERIAL PRIMARY KEY,
    timestamp TIMESTAMPTZ NOT NULL,
    stock_code VARCHAR(10) NOT NULL,
    signal_type VARCHAR(50) NOT NULL,
    confidence DECIMAL(3,1) NOT NULL,
    amount BIGINT,
    investor_type VARCHAR(20),
    detection_method VARCHAR(50),
    metadata JSONB,
    created_at TIMESTAMPTZ DEFAULT NOW()
);

CREATE INDEX idx_smart_money_timestamp ON smart_money_signals (timestamp DESC);
CREATE INDEX idx_smart_money_stock ON smart_money_signals (stock_code);
CREATE INDEX idx_smart_money_confidence ON smart_money_signals (confidence DESC);
```

#### 4.2 ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²° í´ë˜ìŠ¤
**src/utils/database.py**
```python
import asyncpg
import asyncio
from typing import Dict, List, Optional, Any
from contextlib import asynccontextmanager
import logging

class DatabaseManager:
    def __init__(self, database_url: str, pool_size: int = 20):
        self.database_url = database_url
        self.pool_size = pool_size
        self.pool = None
        self.logger = logging.getLogger(__name__)
    
    async def initialize(self):
        """ë°ì´í„°ë² ì´ìŠ¤ í’€ ì´ˆê¸°í™”"""
        try:
            self.pool = await asyncpg.create_pool(
                self.database_url,
                min_size=5,
                max_size=self.pool_size,
                command_timeout=60
            )
            self.logger.info("Database pool initialized successfully")
        except Exception as e:
            self.logger.error(f"Failed to initialize database pool: {e}")
            raise
    
    async def close(self):
        """ë°ì´í„°ë² ì´ìŠ¤ í’€ ì¢…ë£Œ"""
        if self.pool:
            await self.pool.close()
    
    @asynccontextmanager
    async def get_connection(self):
        """ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²° ì»¨í…ìŠ¤íŠ¸ ë§¤ë‹ˆì €"""
        async with self.pool.acquire() as connection:
            yield connection
    
    async def insert_investor_trading(self, data: Dict) -> None:
        """íˆ¬ìì ê±°ë˜ ë°ì´í„° ì‚½ì…"""
        query = """
        INSERT INTO investor_trading (
            timestamp, stock_code, market,
            foreign_buy, foreign_sell, foreign_net,
            institution_buy, institution_sell, institution_net,
            individual_buy, individual_sell, individual_net,
            program_buy, program_sell, program_net
        ) VALUES (
            $1, $2, $3, $4, $5, $6, $7, $8, $9, $10, $11, $12, $13, $14, $15
        )
        ON CONFLICT (timestamp, COALESCE(stock_code, ''), market) 
        DO UPDATE SET
            foreign_buy = EXCLUDED.foreign_buy,
            foreign_sell = EXCLUDED.foreign_sell,
            foreign_net = EXCLUDED.foreign_net,
            institution_buy = EXCLUDED.institution_buy,
            institution_sell = EXCLUDED.institution_sell,
            institution_net = EXCLUDED.institution_net,
            individual_buy = EXCLUDED.individual_buy,
            individual_sell = EXCLUDED.individual_sell,
            individual_net = EXCLUDED.individual_net,
            program_buy = EXCLUDED.program_buy,
            program_sell = EXCLUDED.program_sell,
            program_net = EXCLUDED.program_net,
            updated_at = NOW()
        """
        
        async with self.get_connection() as conn:
            await conn.execute(query, *data.values())
    
    async def get_investor_trading_history(
        self,
        stock_code: Optional[str] = None,
        market: str = "ALL",
        hours: int = 24
    ) -> List[Dict]:
        """íˆ¬ìì ê±°ë˜ ì´ë ¥ ì¡°íšŒ"""
        
        where_conditions = ["timestamp >= NOW() - INTERVAL '%s hours'" % hours]
        params = []
        
        if stock_code:
            where_conditions.append("stock_code = $%d" % (len(params) + 1))
            params.append(stock_code)
            
        if market != "ALL":
            where_conditions.append("market = $%d" % (len(params) + 1))
            params.append(market)
        
        query = f"""
        SELECT * FROM investor_trading
        WHERE {' AND '.join(where_conditions)}
        ORDER BY timestamp DESC
        LIMIT 1000
        """
        
        async with self.get_connection() as conn:
            rows = await conn.fetch(query, *params)
            return [dict(row) for row in rows]
```

---

## ğŸš€ Phase 2: í•µì‹¬ ê¸°ëŠ¥ êµ¬í˜„ (6ì¼)

### Day 5-6: íˆ¬ìì ë§¤ë§¤ ë™í–¥ ë„êµ¬ êµ¬í˜„

#### 5.1 íˆ¬ìì ë§¤ë§¤ ë„êµ¬ í•µì‹¬ ë¡œì§
**src/tools/investor_tools.py**
```python
import asyncio
from typing import Dict, List, Optional
from datetime import datetime, timedelta
import pandas as pd
import numpy as np

from ..api.korea_investment import KoreaInvestmentAPI
from ..api.models import InvestorTradingData, InvestorData, StockInfo
from ..utils.database import DatabaseManager
from ..utils.cache import CacheManager
from ..utils.calculator import TradingCalculator

class InvestorTools:
    def __init__(self, config):
        self.config = config
        self.db_manager = DatabaseManager(config.database.url)
        self.cache_manager = CacheManager(config.cache.redis_url)
        self.calculator = TradingCalculator()
        
    async def get_investor_trading(
        self,
        stock_code: Optional[str] = None,
        investor_type: str = "ALL",
        period: str = "1D",
        market: str = "ALL",
        include_details: bool = True
    ) -> Dict:
        """íˆ¬ììë³„ ë§¤ë§¤ ë™í–¥ ì¡°íšŒ"""
        
        # ìºì‹œ í‚¤ ìƒì„±
        cache_key = f"investor:trading:{stock_code or 'market'}:{investor_type}:{period}:{market}"
        
        # ìºì‹œ í™•ì¸
        cached_data = await self.cache_manager.get(cache_key)
        if cached_data:
            return cached_data
        
        try:
            # 1. APIì—ì„œ ì‹¤ì‹œê°„ ë°ì´í„° ì¡°íšŒ
            api_data = await self._fetch_api_data(stock_code, market)
            
            # 2. íˆìŠ¤í† ë¦¬ì»¬ ë°ì´í„° ì¡°íšŒ
            historical_data = await self._fetch_historical_data(stock_code, market, period)
            
            # 3. ë°ì´í„° í†µí•© ë° ë¶„ì„
            result = await self._analyze_investor_data(
                api_data, historical_data, investor_type, include_details
            )
            
            # 4. ìºì‹œ ì €ì¥
            await self.cache_manager.set(
                cache_key, 
                result, 
                ttl=self.config.cache.ttl_minute
            )
            
            return result
            
        except Exception as e:
            self.logger.error(f"Error in get_investor_trading: {e}")
            raise
    
    async def _fetch_api_data(self, stock_code: Optional[str], market: str) -> Dict:
        """APIì—ì„œ ì‹¤ì‹œê°„ ë°ì´í„° ì¡°íšŒ"""
        
        async with KoreaInvestmentAPI(
            self.config.api.korea_investment_key,
            self.config.api.korea_investment_secret
        ) as api:
            if stock_code:
                return await api.get_investor_trading(stock_code=stock_code)
            else:
                return await api.get_investor_trading(market=market)
    
    async def _fetch_historical_data(
        self, 
        stock_code: Optional[str], 
        market: str, 
        period: str
    ) -> List[Dict]:
        """íˆìŠ¤í† ë¦¬ì»¬ ë°ì´í„° ì¡°íšŒ"""
        
        # ê¸°ê°„ë³„ ì‹œê°„ ì„¤ì •
        period_hours = {
            "1D": 24,
            "5D": 120,
            "20D": 480,
            "60D": 1440
        }
        
        hours = period_hours.get(period, 24)
        
        return await self.db_manager.get_investor_trading_history(
            stock_code=stock_code,
            market=market,
            hours=hours
        )
    
    async def _analyze_investor_data(
        self,
        api_data: Dict,
        historical_data: List[Dict],
        investor_type: str,
        include_details: bool
    ) -> Dict:
        """íˆ¬ìì ë°ì´í„° ë¶„ì„"""
        
        # DataFrame ë³€í™˜
        df = pd.DataFrame(historical_data) if historical_data else pd.DataFrame()
        
        # ë¶„ì„ ê²°ê³¼ êµ¬ì¡°
        result = {
            "timestamp": datetime.now().isoformat(),
            "scope": "STOCK" if api_data.get("stock_code") else "MARKET",
            "investor_data": {},
            "market_impact": {},
            "historical_comparison": {}
        }
        
        # ì¢…ëª© ì •ë³´ (ê°œë³„ ì¢…ëª©ì¸ ê²½ìš°)
        if result["scope"] == "STOCK":
            result["stock_info"] = self._extract_stock_info(api_data)
        
        # íˆ¬ììë³„ ë°ì´í„° ë¶„ì„
        investors = ["foreign", "institution", "individual"] if investor_type == "ALL" else [investor_type.lower()]
        
        for investor in investors:
            result["investor_data"][investor] = await self._analyze_single_investor(
                api_data, df, investor, include_details
            )
        
        # í”„ë¡œê·¸ë¨ ë§¤ë§¤ ë°ì´í„°
        if "program" in api_data:
            result["investor_data"]["program"] = self._extract_program_data(api_data)
        
        # ì‹œì¥ ì˜í–¥ë„ ë¶„ì„
        if include_details:
            result["market_impact"] = self._calculate_market_impact(api_data, df)
            result["historical_comparison"] = self._compare_with_history(api_data, df)
        
        return result
    
    async def _analyze_single_investor(
        self,
        api_data: Dict,
        df: pd.DataFrame,
        investor: str,
        include_details: bool
    ) -> Dict:
        """ê°œë³„ íˆ¬ìì ë¶„ì„"""
        
        # í˜„ì¬ ë°ì´í„° ì¶”ì¶œ
        current_data = self._extract_current_investor_data(api_data, investor)
        
        # ê¸°ë³¸ ë©”íŠ¸ë¦­ ê³„ì‚°
        result = {
            "buy_amount": current_data.get("buy_amount", 0),
            "sell_amount": current_data.get("sell_amount", 0),
            "net_amount": current_data.get("net_amount", 0),
            "buy_volume": current_data.get("buy_volume", 0),
            "sell_volume": current_data.get("sell_volume", 0),
            "net_volume": current_data.get("net_volume", 0),
            "average_buy_price": self.calculator.safe_divide(
                current_data.get("buy_amount", 0),
                current_data.get("buy_volume", 0)
            ),
            "average_sell_price": self.calculator.safe_divide(
                current_data.get("sell_amount", 0),
                current_data.get("sell_volume", 0)
            )
        }
        
        # ì¶”ê°€ ë¶„ì„ (íˆìŠ¤í† ë¦¬ ë°ì´í„°ê°€ ìˆëŠ” ê²½ìš°)
        if include_details and not df.empty:
            result.update({
                "net_ratio": self._calculate_net_ratio(current_data),
                "trend": self._determine_trend(df, investor),
                "intensity": self._calculate_intensity(df, investor),
                "consistency": self._calculate_consistency(df, investor),
                "momentum": self._calculate_momentum(df, investor)
            })
            
            # ê¸°ê´€ íˆ¬ìì ì„¸ë¶€ ë¶„ì„
            if investor == "institution":
                result["sub_categories"] = await self._analyze_institution_subcategories(api_data)
        
        return result
    
    def _calculate_net_ratio(self, data: Dict) -> float:
        """ìˆœë§¤ìˆ˜ ë¹„ìœ¨ ê³„ì‚°"""
        buy_amount = data.get("buy_amount", 0)
        sell_amount = data.get("sell_amount", 0)
        total_amount = buy_amount + sell_amount
        
        if total_amount == 0:
            return 0.0
        
        return (buy_amount / total_amount) * 100
    
    def _determine_trend(self, df: pd.DataFrame, investor: str) -> str:
        """íˆ¬ìì íŠ¸ë Œë“œ íŒì •"""
        if df.empty or len(df) < 5:
            return "NEUTRAL"
        
        recent_net = df[f"{investor}_net"].tail(5)
        
        # ì—°ì† ìˆœë§¤ìˆ˜/ìˆœë§¤ë„ í™•ì¸
        consecutive_buy = (recent_net > 0).all()
        consecutive_sell = (recent_net < 0).all()
        
        if consecutive_buy:
            return "ACCUMULATING"
        elif consecutive_sell:
            return "DISTRIBUTING"
        else:
            # íŠ¸ë Œë“œ ê¸°ìš¸ê¸°ë¡œ íŒì •
            slope = np.polyfit(range(len(recent_net)), recent_net, 1)[0]
            if slope > 0:
                return "ACCUMULATING"
            elif slope < 0:
                return "DISTRIBUTING"
            else:
                return "NEUTRAL"
    
    def _calculate_intensity(self, df: pd.DataFrame, investor: str) -> float:
        """ë§¤ë§¤ ê°•ë„ ê³„ì‚° (1-10 ìŠ¤ì¼€ì¼)"""
        if df.empty:
            return 5.0
        
        # ê±°ë˜ëŸ‰ ê¸°ì¤€ ê°•ë„ ê³„ì‚°
        recent_volume = df[f"{investor}_buy_volume"].tail(5).sum() + df[f"{investor}_sell_volume"].tail(5).sum()
        avg_volume = (df[f"{investor}_buy_volume"].mean() + df[f"{investor}_sell_volume"].mean()) * 5
        
        if avg_volume == 0:
            return 5.0
        
        intensity_ratio = recent_volume / avg_volume
        
        # 1-10 ìŠ¤ì¼€ì¼ë¡œ ì •ê·œí™”
        return min(10.0, max(1.0, intensity_ratio * 5))
    
    def _calculate_market_impact(self, api_data: Dict, df: pd.DataFrame) -> Dict:
        """ì‹œì¥ ì˜í–¥ë„ ë¶„ì„"""
        
        if df.empty or len(df) < 2:
            return {
                "price_correlation": 0.0,
                "volume_contribution": 0.0,
                "momentum_score": 0.0
            }
        
        # ê°€ê²© ìƒê´€ê´€ê³„ (ì™¸êµ­ì¸ ìˆœë§¤ìˆ˜ì™€ ê°€ê²© ë³€ë™)
        foreign_net = df["foreign_net"]
        price_changes = df["close"].pct_change() if "close" in df.columns else pd.Series([0] * len(df))
        
        price_correlation = foreign_net.corr(price_changes) if len(foreign_net) > 1 else 0.0
        
        # ê±°ë˜ëŸ‰ ê¸°ì—¬ë„
        total_volume = df["foreign_buy_volume"].sum() + df["foreign_sell_volume"].sum()
        market_volume = df["total_volume"].sum() if "total_volume" in df.columns else total_volume * 3
        volume_contribution = (total_volume / market_volume * 100) if market_volume > 0 else 0.0
        
        # ëª¨ë©˜í…€ ì ìˆ˜
        momentum_score = self._calculate_momentum_score(df)
        
        return {
            "price_correlation": round(price_correlation, 2),
            "volume_contribution": round(volume_contribution, 1),
            "momentum_score": round(momentum_score, 1)
        }
```

### Day 7-8: í”„ë¡œê·¸ë¨ ë§¤ë§¤ ë° ë³´ìœ ë¹„ì¤‘ ë„êµ¬

#### 7.1 í”„ë¡œê·¸ë¨ ë§¤ë§¤ ë„êµ¬
**src/tools/program_tools.py**
```python
from typing import Dict, List, Optional
from datetime import datetime
import pandas as pd

class ProgramTools:
    def __init__(self, config):
        self.config = config
        self.api_clients = self._initialize_api_clients()
        
    async def get_program_trading(
        self,
        market: str = "ALL",
        program_type: str = "ALL",
        time_window: str = "CURRENT"
    ) -> Dict:
        """í”„ë¡œê·¸ë¨ ë§¤ë§¤ ë™í–¥ ì¡°íšŒ"""
        
        cache_key = f"program:trading:{market}:{program_type}:{time_window}"
        
        # ìºì‹œ í™•ì¸
        cached_data = await self.cache_manager.get(cache_key)
        if cached_data:
            return cached_data
        
        try:
            # API ë°ì´í„° ì¡°íšŒ
            api_data = await self._fetch_program_data(market, time_window)
            
            # ë°ì´í„° ë¶„ì„ ë° ê°€ê³µ
            result = await self._analyze_program_data(api_data, program_type, time_window)
            
            # ìºì‹œ ì €ì¥
            await self.cache_manager.set(cache_key, result, ttl=30)  # 30ì´ˆ ìºì‹œ
            
            return result
            
        except Exception as e:
            self.logger.error(f"Error in get_program_trading: {e}")
            raise
    
    async def _analyze_program_data(
        self,
        api_data: Dict,
        program_type: str,
        time_window: str
    ) -> Dict:
        """í”„ë¡œê·¸ë¨ ë§¤ë§¤ ë°ì´í„° ë¶„ì„"""
        
        result = {
            "timestamp": datetime.now().isoformat(),
            "market": api_data.get("market", "ALL"),
            "program_trading": {
                "summary": self._calculate_program_summary(api_data),
                "arbitrage": self._extract_arbitrage_data(api_data),
                "non_arbitrage": self._extract_non_arbitrage_data(api_data),
                "time_series": self._build_time_series(api_data, time_window),
                "top_stocks": self._identify_top_program_stocks(api_data),
                "market_indicators": self._calculate_market_indicators(api_data)
            }
        }
        
        return result
    
    def _calculate_program_summary(self, data: Dict) -> Dict:
        """í”„ë¡œê·¸ë¨ ë§¤ë§¤ ìš”ì•½ ê³„ì‚°"""
        
        total_buy = data.get("total_program_buy", 0)
        total_sell = data.get("total_program_sell", 0)
        net_value = total_buy - total_sell
        
        total_trading = total_buy + total_sell
        buy_ratio = (total_buy / total_trading * 100) if total_trading > 0 else 50.0
        
        # ì‹œì¥ ì˜í–¥ë„ ê³„ì‚°
        market_volume = data.get("total_market_volume", total_trading * 5)  # ì¶”ì •
        market_impact = (total_trading / market_volume * 100) if market_volume > 0 else 0.0
        
        return {
            "total_buy": total_buy,
            "total_sell": total_sell,
            "net_value": net_value,
            "buy_ratio": round(buy_ratio, 1),
            "market_impact": round(market_impact, 1)
        }
    
    def _extract_arbitrage_data(self, data: Dict) -> Dict:
        """ì°¨ìµê±°ë˜ ë°ì´í„° ì¶”ì¶œ"""
        
        arb_buy = data.get("arbitrage_buy", 0)
        arb_sell = data.get("arbitrage_sell", 0)
        
        # ë² ì´ì‹œìŠ¤ ê³„ì‚° (ì„ ë¬¼-í˜„ë¬¼ ê°€ê²©ì°¨)
        basis = data.get("futures_basis", 0)
        
        # ì„ ë¬¼ í¬ì§€ì…˜ ì¶”ì •
        futures_position = "LONG" if arb_buy > arb_sell else "SHORT" if arb_sell > arb_buy else "NEUTRAL"
        
        # ì°¨ìµê±°ë˜ ê°•ë„
        arb_volume = arb_buy + arb_sell
        intensity = "HIGH" if arb_volume > 1e11 else "MEDIUM" if arb_volume > 5e10 else "LOW"
        
        return {
            "buy": arb_buy,
            "sell": arb_sell,
            "net": arb_buy - arb_sell,
            "basis": round(basis, 1),
            "futures_position": futures_position,
            "intensity": intensity
        }
```

#### 7.2 ë³´ìœ ë¹„ì¤‘ ì¶”ì  ë„êµ¬
**src/tools/ownership_tools.py**
```python
class OwnershipTools:
    def __init__(self, config):
        self.config = config
        
    async def get_ownership_changes(
        self,
        stock_code: str,
        investor_type: str = "ALL",
        period: str = "3M",
        threshold: float = 1.0
    ) -> Dict:
        """íˆ¬ììë³„ ë³´ìœ  ë¹„ì¤‘ ë³€í™” ì¶”ì """
        
        cache_key = f"ownership:{stock_code}:{investor_type}:{period}"
        
        try:
            # ë³´ìœ ë¹„ì¤‘ ë°ì´í„° ì¡°íšŒ
            ownership_data = await self._fetch_ownership_data(stock_code, period)
            
            # ë³€í™” ë¶„ì„
            result = await self._analyze_ownership_changes(
                ownership_data, stock_code, investor_type, threshold
            )
            
            return result
            
        except Exception as e:
            self.logger.error(f"Error in get_ownership_changes: {e}")
            raise
    
    async def _analyze_ownership_changes(
        self,
        ownership_data: List[Dict],
        stock_code: str,
        investor_type: str,
        threshold: float
    ) -> Dict:
        """ë³´ìœ ë¹„ì¤‘ ë³€í™” ë¶„ì„"""
        
        if not ownership_data:
            return {"error": "No ownership data available"}
        
        # ìµœì‹  ë°ì´í„°
        latest = ownership_data[0]
        
        # ë³€í™” ê³„ì‚°
        changes = self._calculate_ownership_changes(ownership_data, threshold)
        
        # íˆìŠ¤í† ë¦¬ì»¬ ë°ì´í„° êµ¬ì„±
        historical = self._build_ownership_history(ownership_data)
        
        # ë§ˆì¼ìŠ¤í†¤ ì´ë²¤íŠ¸ ì‹ë³„
        milestones = self._identify_ownership_milestones(ownership_data)
        
        # ìƒê´€ê´€ê³„ ë¶„ì„
        correlation = self._analyze_ownership_correlation(ownership_data, stock_code)
        
        result = {
            "timestamp": datetime.now().isoformat(),
            "stock_info": {
                "code": stock_code,
                "name": latest.get("stock_name", ""),
                "market_cap": latest.get("market_cap", 0)
            },
            "ownership_data": {
                "current": self._extract_current_ownership(latest),
                "changes": changes,
                "historical": historical,
                "milestones": milestones
            },
            "correlation_analysis": correlation
        }
        
        return result
    
    def _calculate_ownership_changes(self, data: List[Dict], threshold: float) -> Dict:
        """ë³´ìœ ë¹„ì¤‘ ë³€í™” ê³„ì‚°"""
        
        if len(data) < 2:
            return {}
        
        latest = data[0]
        previous = data[-1]  # ê°€ì¥ ì˜¤ë˜ëœ ë°ì´í„°
        
        changes = {}
        
        for investor_type in ["foreign", "institution", "individual"]:
            current_pct = latest.get(f"{investor_type}_ownership_pct", 0)
            previous_pct = previous.get(f"{investor_type}_ownership_pct", 0)
            
            pct_change = current_pct - previous_pct
            
            if abs(pct_change) >= threshold:
                # ì—°ì†ì¼ ê³„ì‚°
                consecutive_days = self._calculate_consecutive_days(data, investor_type)
                
                changes[investor_type] = {
                    "shares_change": latest.get(f"{investor_type}_shares", 0) - previous.get(f"{investor_type}_shares", 0),
                    "percentage_change": round(pct_change, 2),
                    "value_change": self._calculate_value_change(latest, previous, investor_type),
                    "trend": "INCREASING" if pct_change > 0 else "DECREASING",
                    "consecutive_days": consecutive_days
                }
        
        return changes
```

### Day 9-10: ìŠ¤ë§ˆíŠ¸ë¨¸ë‹ˆ ì¶”ì  ë° ë¶„ì„ ì—”ì§„

#### 9.1 ìŠ¤ë§ˆíŠ¸ë¨¸ë‹ˆ ì¶”ì ê¸° í•µì‹¬ êµ¬í˜„
**src/analysis/smart_money.py**
```python
import pandas as pd
import numpy as np
from typing import Dict, List, Tuple, Optional
from datetime import datetime, timedelta
from sklearn.cluster import DBSCAN
from sklearn.preprocessing import StandardScaler

class SmartMoneyTracker:
    def __init__(self, config):
        self.config = config
        self.smart_money_threshold = config.analysis.smart_money_threshold
        self.large_order_threshold = config.analysis.large_order_threshold
        
    async def track_smart_money(
        self,
        detection_method: str = "LARGE_ORDERS",
        market: str = "ALL",
        min_confidence: float = 7.0,
        count: int = 20
    ) -> Dict:
        """ìŠ¤ë§ˆíŠ¸ë¨¸ë‹ˆ ì¶”ì """
        
        try:
            # 1. ì›ì‹œ ë°ì´í„° ìˆ˜ì§‘
            raw_data = await self._collect_raw_data(market)
            
            # 2. ê°ì§€ ë°©ë²•ë³„ ë¶„ì„
            signals = await self._detect_smart_money_signals(
                raw_data, detection_method, min_confidence
            )
            
            # 3. ì‹œì¥ ìŠ¤ë§ˆíŠ¸ë¨¸ë‹ˆ ì§€ìˆ˜ ê³„ì‚°
            market_index = self._calculate_market_smart_money_index(signals)
            
            # 4. ê¸°ê´€ í¬ì§€ì…”ë‹ ë¶„ì„
            institutional_positioning = self._analyze_institutional_positioning(raw_data)
            
            # 5. ê²°ê³¼ êµ¬ì„±
            result = {
                "timestamp": datetime.now().isoformat(),
                "smart_money_signals": signals[:count],
                "market_smart_money_index": market_index,
                "institutional_positioning": institutional_positioning,
                "detection_method": detection_method,
                "total_signals_found": len(signals)
            }
            
            return result
            
        except Exception as e:
            self.logger.error(f"Error in track_smart_money: {e}")
            raise
    
    async def _detect_smart_money_signals(
        self,
        raw_data: Dict,
        detection_method: str,
        min_confidence: float
    ) -> List[Dict]:
        """ìŠ¤ë§ˆíŠ¸ë¨¸ë‹ˆ ì‹ í˜¸ ê°ì§€"""
        
        signals = []
        
        if detection_method == "LARGE_ORDERS":
            signals.extend(await self._detect_large_block_orders(raw_data))
        elif detection_method == "INSTITUTION_CLUSTER":
            signals.extend(await self._detect_institutional_clustering(raw_data))
        elif detection_method == "FOREIGN_SURGE":
            signals.extend(await self._detect_foreign_surge_patterns(raw_data))
        else:
            # ëª¨ë“  ë°©ë²• ì ìš©
            signals.extend(await self._detect_large_block_orders(raw_data))
            signals.extend(await self._detect_institutional_clustering(raw_data))
            signals.extend(await self._detect_foreign_surge_patterns(raw_data))
        
        # ì‹ ë¢°ë„ í•„í„°ë§ ë° ì •ë ¬
        filtered_signals = [s for s in signals if s["confidence"] >= min_confidence]
        return sorted(filtered_signals, key=lambda x: x["confidence"], reverse=True)
    
    async def _detect_large_block_orders(self, data: Dict) -> List[Dict]:
        """ëŒ€í˜• ë¸”ë¡ ì£¼ë¬¸ ê°ì§€"""
        
        signals = []
        
        for stock_code, stock_data in data.items():
            if not isinstance(stock_data, dict):
                continue
                
            # ëŒ€í˜• ê±°ë˜ ì‹ë³„
            large_trades = self._identify_large_trades(stock_data)
            
            if len(large_trades) >= 3:  # ìµœì†Œ 3ê±´ ì´ìƒì˜ ëŒ€í˜• ê±°ë˜
                
                # ì‹ ë¢°ë„ ê³„ì‚°
                confidence = self._calculate_block_trade_confidence(large_trades, stock_data)
                
                if confidence >= 5.0:  # ìµœì†Œ ì‹ ë¢°ë„
                    
                    signal = {
                        "stock_code": stock_code,
                        "stock_name": stock_data.get("name", ""),
                        "signal_type": "LARGE_BLOCK_ACCUMULATION",
                        "confidence": confidence,
                        "detection_details": {
                            "large_block_trades": len(large_trades),
                            "average_block_size": np.mean([t["amount"] for t in large_trades]),
                            "institutional_buyers": self._identify_institutional_buyers(large_trades),
                            "accumulation_period": self._calculate_accumulation_period(large_trades),
                            "price_impact": self._calculate_price_impact(large_trades, stock_data)
                        },
                        "metrics": self._calculate_block_trade_metrics(large_trades, stock_data),
                        "technical_context": self._get_technical_context(stock_data),
                        "similar_patterns": await self._find_similar_patterns(stock_code, large_trades)
                    }
                    
                    signals.append(signal)
        
        return signals
    
    def _calculate_block_trade_confidence(self, large_trades: List[Dict], stock_data: Dict) -> float:
        """ë¸”ë¡ ê±°ë˜ ì‹ ë¢°ë„ ê³„ì‚°"""
        
        # ê¸°ë³¸ ì ìˆ˜
        base_score = 5.0
        
        # ê±°ë˜ ê±´ìˆ˜ ê°€ì‚°ì 
        trade_count_bonus = min(len(large_trades) * 0.5, 3.0)
        
        # í‰ê·  ê±°ë˜ ê·œëª¨ ê°€ì‚°ì 
        avg_size = np.mean([t["amount"] for t in large_trades])
        size_bonus = min((avg_size / self.large_order_threshold - 1) * 2, 2.0)
        
        # ê°€ê²© ì˜í–¥ ìµœì†Œí™” ê°€ì‚°ì  (ìŠ¤í…”ìŠ¤ ì¶•ì )
        price_impact = self._calculate_price_impact(large_trades, stock_data)
        stealth_bonus = max(0, 2.0 - price_impact) if price_impact < 2.0 else 0
        
        # ê¸°ê´€ íˆ¬ìì ë‹¤ì–‘ì„± ê°€ì‚°ì 
        institutional_types = set([t.get("investor_type") for t in large_trades])
        diversity_bonus = min(len(institutional_types) * 0.3, 1.0)
        
        # ì‹œê°„ ì§‘ì¤‘ë„ ê°€ì‚°ì 
        time_concentration = self._calculate_time_concentration(large_trades)
        timing_bonus = min(time_concentration * 0.5, 1.0)
        
        total_confidence = (
            base_score + 
            trade_count_bonus + 
            size_bonus + 
            stealth_bonus + 
            diversity_bonus + 
            timing_bonus
        )
        
        return min(total_confidence, 10.0)
    
    async def _detect_institutional_clustering(self, data: Dict) -> List[Dict]:
        """ê¸°ê´€ íˆ¬ìì í´ëŸ¬ìŠ¤í„°ë§ ê°ì§€"""
        
        signals = []
        
        # ê¸°ê´€ íˆ¬ììë³„ ë§¤ë§¤ íŒ¨í„´ ë¶„ì„
        institutional_patterns = self._analyze_institutional_patterns(data)
        
        # í´ëŸ¬ìŠ¤í„°ë§ ë¶„ì„
        clusters = self._perform_institutional_clustering(institutional_patterns)
        
        for cluster in clusters:
            if cluster["signal_strength"] >= 7.0:
                
                signal = {
                    "stock_code": cluster["primary_stock"],
                    "stock_name": data.get(cluster["primary_stock"], {}).get("name", ""),
                    "signal_type": "INSTITUTIONAL_CLUSTERING",
                    "confidence": cluster["signal_strength"],
                    "detection_details": {
                        "participating_institutions": cluster["institutions"],
                        "cluster_size": len(cluster["stocks"]),
                        "coordination_score": cluster["coordination_score"],
                        "time_synchronization": cluster["time_sync"],
                        "sector_focus": cluster["sector_focus"]
                    },
                    "metrics": cluster["metrics"],
                    "technical_context": self._get_technical_context(
                        data.get(cluster["primary_stock"], {})
                    )
                }
                
                signals.append(signal)
        
        return signals
    
    def _analyze_institutional_patterns(self, data: Dict) -> Dict:
        """ê¸°ê´€ íˆ¬ìì íŒ¨í„´ ë¶„ì„"""
        
        patterns = {}
        
        for stock_code, stock_data in data.items():
            if not isinstance(stock_data, dict):
                continue
            
            # ê¸°ê´€ë³„ ê±°ë˜ íŒ¨í„´ ì¶”ì¶œ
            institutional_data = stock_data.get("institutional_trades", [])
            
            if institutional_data:
                patterns[stock_code] = {
                    "pension_fund_pattern": self._extract_investor_pattern(institutional_data, "pension_fund"),
                    "investment_trust_pattern": self._extract_investor_pattern(institutional_data, "investment_trust"),
                    "insurance_pattern": self._extract_investor_pattern(institutional_data, "insurance"),
                    "bank_pattern": self._extract_investor_pattern(institutional_data, "bank"),
                    "coordination_indicators": self._calculate_coordination_indicators(institutional_data)
                }
        
        return patterns
    
    def _perform_institutional_clustering(self, patterns: Dict) -> List[Dict]:
        """ê¸°ê´€ íˆ¬ìì í´ëŸ¬ìŠ¤í„°ë§ ìˆ˜í–‰"""
        
        if not patterns:
            return []
        
        # íŠ¹ì§• ë²¡í„° ìƒì„±
        features = []
        stock_codes = []
        
        for stock_code, pattern in patterns.items():
            feature_vector = self._create_feature_vector(pattern)
            features.append(feature_vector)
            stock_codes.append(stock_code)
        
        if len(features) < 3:
            return []
        
        # ì •ê·œí™”
        scaler = StandardScaler()
        features_scaled = scaler.fit_transform(features)
        
        # DBSCAN í´ëŸ¬ìŠ¤í„°ë§
        clustering = DBSCAN(eps=0.5, min_samples=2)
        labels = clustering.fit_predict(features_scaled)
        
        # í´ëŸ¬ìŠ¤í„° ë¶„ì„
        clusters = []
        unique_labels = set(labels)
        
        for label in unique_labels:
            if label == -1:  # ë…¸ì´ì¦ˆ ì œì™¸
                continue
                
            cluster_indices = [i for i, l in enumerate(labels) if l == label]
            cluster_stocks = [stock_codes[i] for i in cluster_indices]
            
            cluster_analysis = self._analyze_cluster(
                cluster_stocks, 
                {stock: patterns[stock] for stock in cluster_stocks}
            )
            
            if cluster_analysis["signal_strength"] >= 6.0:
                clusters.append(cluster_analysis)
        
        return sorted(clusters, key=lambda x: x["signal_strength"], reverse=True)
    
    def _calculate_market_smart_money_index(self, signals: List[Dict]) -> Dict:
        """ì‹œì¥ ìŠ¤ë§ˆíŠ¸ë¨¸ë‹ˆ ì§€ìˆ˜ ê³„ì‚°"""
        
        if not signals:
            return {
                "current_value": 50.0,
                "trend": "NEUTRAL",
                "interpretation": "No significant smart money activity",
                "sector_focus": [],
                "risk_appetite": "NEUTRAL"
            }
        
        # ì‹ ë¢°ë„ ê°€ì¤‘ í‰ê· 
        weighted_confidence = np.average(
            [s["confidence"] for s in signals],
            weights=[s["confidence"] for s in signals]
        )
        
        # ì‹œì¥ ì§€ìˆ˜ ê³„ì‚° (0-100)
        market_index = min(weighted_confidence * 10, 100)
        
        # íŠ¸ë Œë“œ ë¶„ì„
        recent_signals = [s for s in signals if self._is_recent_signal(s)]
        trend = self._determine_market_trend(recent_signals)
        
        # ì„¹í„° ì§‘ì¤‘ë„ ë¶„ì„
        sector_focus = self._analyze_sector_concentration(signals)
        
        # ìœ„í—˜ ì„ í˜¸ë„ ë¶„ì„
        risk_appetite = self._analyze_risk_appetite(signals)
        
        return {
            "current_value": round(market_index, 1),
            "trend": trend,
            "interpretation": self._interpret_market_index(market_index),
            "sector_focus": sector_focus[:5],  # ìƒìœ„ 5ê°œ ì„¹í„°
            "risk_appetite": risk_appetite
        }
```

---

## âš¡ Phase 3: ê³ ê¸‰ ê¸°ëŠ¥ ë° ìµœì í™” (5ì¼)

### Day 11-12: ì‹¤ì‹œê°„ ë°ì´í„° ì²˜ë¦¬ ë° ë¶„ì„ ì—”ì§„

#### 11.1 ì‹¤ì‹œê°„ ë°ì´í„° ìŠ¤íŠ¸ë¦¬ë°
**src/utils/realtime_processor.py**
```python
import asyncio
import websockets
import json
from typing import Dict, List, Callable, Optional
from datetime import datetime
import aioredis
from dataclasses import dataclass

@dataclass
class StreamingData:
    timestamp: datetime
    stock_code: str
    data_type: str  # TRADE, QUOTE, ORDER
    data: Dict

class RealTimeProcessor:
    def __init__(self, config):
        self.config = config
        self.subscribers = {}
        self.processing_queue = asyncio.Queue(maxsize=10000)
        self.redis_client = None
        self.running = False
        
    async def start(self):
        """ì‹¤ì‹œê°„ ì²˜ë¦¬ ì‹œì‘"""
        self.redis_client = await aioredis.from_url(self.config.cache.redis_url)
        self.running = True
        
        # ì²˜ë¦¬ íƒœìŠ¤í¬ë“¤ ì‹œì‘
        await asyncio.gather(
            self._websocket_listener(),
            self._data_processor(),
            self._anomaly_detector(),
            self._signal_generator()
        )
    
    async def _websocket_listener(self):
        """ì›¹ì†Œì¼“ ë°ì´í„° ìˆ˜ì‹ """
        
        while self.running:
            try:
                async with websockets.connect(
                    "wss://api.example.com/realtime",
                    extra_headers={"Authorization": f"Bearer {self.access_token}"}
                ) as websocket:
                    
                    # êµ¬ë… ì„¤ì •
                    subscribe_msg = {
                        "action": "subscribe",
                        "channels": ["investor_trading", "program_trading", "large_orders"]
                    }
                    await websocket.send(json.dumps(subscribe_msg))
                    
                    async for message in websocket:
                        data = json.loads(message)
                        stream_data = StreamingData(
                            timestamp=datetime.now(),
                            stock_code=data.get("stock_code", ""),
                            data_type=data.get("type", "UNKNOWN"),
                            data=data
                        )
                        
                        await self.processing_queue.put(stream_data)
                        
            except Exception as e:
                self.logger.error(f"WebSocket connection error: {e}")
                await asyncio.sleep(5)  # ì¬ì—°ê²° ëŒ€ê¸°
    
    async def _data_processor(self):
        """ë°ì´í„° ì²˜ë¦¬ ì›Œì»¤"""
        
        while self.running:
            try:
                # ë°°ì¹˜ ì²˜ë¦¬ë¥¼ ìœ„í•´ ì—¬ëŸ¬ ë°ì´í„° ìˆ˜ì§‘
                batch = []
                timeout = 0.1  # 100ms ëŒ€ê¸°
                
                try:
                    # ì²« ë²ˆì§¸ ë°ì´í„° ëŒ€ê¸°
                    first_data = await asyncio.wait_for(
                        self.processing_queue.get(), 
                        timeout=timeout
                    )
                    batch.append(first_data)
                    
                    # ì¶”ê°€ ë°ì´í„° ìˆ˜ì§‘ (ë…¼ë¸”ë¡œí‚¹)
                    while len(batch) < 100:  # ìµœëŒ€ 100ê°œê¹Œì§€ ë°°ì¹˜
                        try:
                            data = self.processing_queue.get_nowait()
                            batch.append(data)
                        except asyncio.QueueEmpty:
                            break
                            
                except asyncio.TimeoutError:
                    continue
                
                if batch:
                    await self._process_batch(batch)
                    
            except Exception as e:
                self.logger.error(f"Data processing error: {e}")
    
    async def _process_batch(self, batch: List[StreamingData]):
        """ë°°ì¹˜ ë°ì´í„° ì²˜ë¦¬"""
        
        # íƒ€ì…ë³„ë¡œ ê·¸ë£¹í™”
        grouped_data = {}
        for data in batch:
            data_type = data.data_type
            if data_type not in grouped_data:
                grouped_data[data_type] = []
            grouped_data[data_type].append(data)
        
        # íƒ€ì…ë³„ ì²˜ë¦¬
        for data_type, data_list in grouped_data.items():
            if data_type == "INVESTOR_TRADE":
                await self._process_investor_trades(data_list)
            elif data_type == "PROGRAM_TRADE":
                await self._process_program_trades(data_list)
            elif data_type == "LARGE_ORDER":
                await self._process_large_orders(data_list)
    
    async def _process_investor_trades(self, trades: List[StreamingData]):
        """íˆ¬ìì ê±°ë˜ ë°ì´í„° ì²˜ë¦¬"""
        
        # ë°ì´í„°ë² ì´ìŠ¤ ì €ì¥ì„ ìœ„í•œ ë°°ì¹˜ ì¤€ë¹„
        db_records = []
        
        for trade in trades:
            data = trade.data
            
            record = {
                "timestamp": trade.timestamp,
                "stock_code": trade.stock_code,
                "foreign_buy": data.get("foreign_buy", 0),
                "foreign_sell": data.get("foreign_sell", 0),
                "institution_buy": data.get("institution_buy", 0),
                "institution_sell": data.get("institution_sell", 0),
                "individual_buy": data.get("individual_buy", 0),
                "individual_sell": data.get("individual_sell", 0)
            }
            
            db_records.append(record)
            
            # ì‹¤ì‹œê°„ ë¶„ì„
            await self._analyze_trade_real_time(trade)
        
        # ë°°ì¹˜ ì €ì¥
        if db_records:
            await self.db_manager.batch_insert_investor_trading(db_records)
    
    async def _analyze_trade_real_time(self, trade: StreamingData):
        """ì‹¤ì‹œê°„ ê±°ë˜ ë¶„ì„"""
        
        data = trade.data
        
        # ì´ìƒ ê±°ë˜ ê°ì§€
        anomalies = await self._detect_trade_anomalies(trade)
        
        if anomalies:
            # ì•Œë¦¼ ë°œì†¡
            for anomaly in anomalies:
                await self._send_real_time_alert(anomaly)
        
        # íŒ¨í„´ ì—…ë°ì´íŠ¸
        await self._update_patterns_real_time(trade)
        
        # ìŠ¤ë§ˆíŠ¸ë¨¸ë‹ˆ ì‹ í˜¸ ì²´í¬
        smart_money_signals = await self._check_smart_money_signals(trade)
        
        if smart_money_signals:
            # Redisì— ì‹ í˜¸ ì €ì¥
            await self._cache_smart_money_signals(smart_money_signals)
    
    async def _anomaly_detector(self):
        """ì´ìƒ ê±°ë˜ ê°ì§€ ì—”ì§„"""
        
        while self.running:
            try:
                # ìµœê·¼ ë°ì´í„° ìˆ˜ì§‘
                recent_data = await self._get_recent_trading_data()
                
                # í†µê³„ì  ì´ìƒ ê°ì§€
                statistical_anomalies = self._detect_statistical_anomalies(recent_data)
                
                # íŒ¨í„´ ê¸°ë°˜ ì´ìƒ ê°ì§€
                pattern_anomalies = self._detect_pattern_anomalies(recent_data)
                
                # ML ê¸°ë°˜ ì´ìƒ ê°ì§€
                ml_anomalies = await self._detect_ml_anomalies(recent_data)
                
                # ëª¨ë“  ì´ìƒ ê±°ë˜ í†µí•©
                all_anomalies = statistical_anomalies + pattern_anomalies + ml_anomalies
                
                # ì¤‘ìš”ë„ ì •ë ¬ ë° ì²˜ë¦¬
                sorted_anomalies = sorted(all_anomalies, key=lambda x: x["severity"], reverse=True)
                
                for anomaly in sorted_anomalies[:10]:  # ìƒìœ„ 10ê°œë§Œ ì²˜ë¦¬
                    await self._handle_anomaly(anomaly)
                
                await asyncio.sleep(30)  # 30ì´ˆë§ˆë‹¤ ê²€ì‚¬
                
            except Exception as e:
                self.logger.error(f"Anomaly detection error: {e}")
                await asyncio.sleep(60)
```

#### 11.2 ML ê¸°ë°˜ íŒ¨í„´ ê°ì§€
**src/analysis/ml_detector.py**
```python
import numpy as np
import pandas as pd
from sklearn.ensemble import IsolationForest
from sklearn.preprocessing import StandardScaler
from sklearn.cluster import DBSCAN
import joblib
from typing import Dict, List, Tuple, Optional

class MLPatternDetector:
    def __init__(self, config):
        self.config = config
        self.models = {}
        self.scalers = {}
        self.feature_extractors = {}
        
    async def initialize_models(self):
        """ML ëª¨ë¸ ì´ˆê¸°í™”"""
        
        # ì´ìƒ ê±°ë˜ ê°ì§€ ëª¨ë¸
        self.models["anomaly"] = IsolationForest(
            contamination=0.1,
            random_state=42,
            n_estimators=200
        )
        
        # íŒ¨í„´ í´ëŸ¬ìŠ¤í„°ë§ ëª¨ë¸
        self.models["clustering"] = DBSCAN(
            eps=0.5,
            min_samples=5
        )
        
        # ìŠ¤ì¼€ì¼ëŸ¬ ì´ˆê¸°í™”
        self.scalers["trading"] = StandardScaler()
        self.scalers["price"] = StandardScaler()
        
        # ê¸°ì¡´ ëª¨ë¸ ë¡œë“œ ì‹œë„
        await self._load_pretrained_models()
        
    async def _load_pretrained_models(self):
        """ì‚¬ì „ í›ˆë ¨ëœ ëª¨ë¸ ë¡œë“œ"""
        
        try:
            self.models["anomaly"] = joblib.load("models/anomaly_detector.pkl")
            self.scalers["trading"] = joblib.load("models/trading_scaler.pkl")
            self.logger.info("Pre-trained models loaded successfully")
        except FileNotFoundError:
            self.logger.info("No pre-trained models found, using fresh models")
    
    async def detect_anomalies(self, trading_data: pd.DataFrame) -> List[Dict]:
        """ML ê¸°ë°˜ ì´ìƒ ê±°ë˜ ê°ì§€"""
        
        if trading_data.empty or len(trading_data) < 10:
            return []
        
        # íŠ¹ì§• ì¶”ì¶œ
        features = self._extract_trading_features(trading_data)
        
        if features.empty:
            return []
        
        # ë°ì´í„° ì •ê·œí™”
        features_scaled = self.scalers["trading"].fit_transform(features)
        
        # ì´ìƒ ê°ì§€
        anomaly_scores = self.models["anomaly"].decision_function(features_scaled)
        anomaly_labels = self.models["anomaly"].predict(features_scaled)
        
        # ì´ìƒ ê±°ë˜ ì‹ë³„
        anomalies = []
        anomaly_indices = np.where(anomaly_labels == -1)[0]
        
        for idx in anomaly_indices:
            anomaly_data = trading_data.iloc[idx]
            
            anomaly = {
                "timestamp": anomaly_data["timestamp"],
                "stock_code": anomaly_data.get("stock_code", "MARKET"),
                "anomaly_type": self._classify_anomaly_type(features.iloc[idx]),
                "severity": self._calculate_anomaly_severity(anomaly_scores[idx]),
                "details": {
                    "anomaly_score": float(anomaly_scores[idx]),
                    "feature_contributions": self._analyze_feature_contributions(features.iloc[idx]),
                    "trading_metrics": {
                        "foreign_net": int(anomaly_data.get("foreign_net", 0)),
                        "institution_net": int(anomaly_data.get("institution_net", 0)),
                        "volume_spike": self._calculate_volume_spike(anomaly_data)
                    }
                },
                "confidence": self._calculate_anomaly_confidence(anomaly_scores[idx])
            }
            
            anomalies.append(anomaly)
        
        return sorted(anomalies, key=lambda x: x["severity"], reverse=True)
    
    def _extract_trading_features(self, data: pd.DataFrame) -> pd.DataFrame:
        """ê±°ë˜ ë°ì´í„°ì—ì„œ íŠ¹ì§• ì¶”ì¶œ"""
        
        features = pd.DataFrame()
        
        # ê¸°ë³¸ ê±°ë˜ íŠ¹ì§•
        features["foreign_net_ratio"] = data["foreign_net"] / (data["foreign_buy"] + data["foreign_sell"] + 1)
        features["institution_net_ratio"] = data["institution_net"] / (data["institution_buy"] + data["institution_sell"] + 1)
        features["individual_net_ratio"] = data["individual_net"] / (data["individual_buy"] + data["individual_sell"] + 1)
        
        # ê±°ë˜ëŸ‰ íŠ¹ì§•
        features["total_volume"] = data["foreign_buy_volume"] + data["foreign_sell_volume"] + \
                                 data["institution_buy_volume"] + data["institution_sell_volume"] + \
                                 data["individual_buy_volume"] + data["individual_sell_volume"]
        
        # ì´ë™í‰ê·  ê¸°ë°˜ íŠ¹ì§• (ë¡¤ë§ ìœˆë„ìš°)
        window = min(20, len(data))
        if window > 1:
            features["foreign_net_ma"] = data["foreign_net"].rolling(window).mean()
            features["volume_ma"] = features["total_volume"].rolling(window).mean()
            
            # ì´ë™í‰ê·  ëŒ€ë¹„ í¸ì°¨
            features["foreign_net_deviation"] = (data["foreign_net"] - features["foreign_net_ma"]) / (features["foreign_net_ma"].abs() + 1)
            features["volume_deviation"] = (features["total_volume"] - features["volume_ma"]) / (features["volume_ma"] + 1)
        
        # ë³€í™”ìœ¨ íŠ¹ì§•
        features["foreign_net_change"] = data["foreign_net"].pct_change().fillna(0)
        features["institution_net_change"] = data["institution_net"].pct_change().fillna(0)
        
        # ìƒí˜¸ì‘ìš© íŠ¹ì§•
        features["foreign_institution_alignment"] = np.sign(data["foreign_net"]) * np.sign(data["institution_net"])
        features["smart_money_flow"] = (data["foreign_net"] + data["institution_net"]) / (features["total_volume"] + 1)
        
        # ì‹œê°„ ê¸°ë°˜ íŠ¹ì§•
        if "timestamp" in data.columns:
            data["hour"] = pd.to_datetime(data["timestamp"]).dt.hour
            features["is_opening"] = (data["hour"] == 9).astype(int)
            features["is_closing"] = (data["hour"] >= 14).astype(int)
        
        # NaN ê°’ ì²˜ë¦¬
        features = features.fillna(0)
        
        return features
    
    def _classify_anomaly_type(self, feature_row: pd.Series) -> str:
        """ì´ìƒ ê±°ë˜ ìœ í˜• ë¶„ë¥˜"""
        
        # ì™¸êµ­ì¸ ëŒ€ëŸ‰ ê±°ë˜
        if abs(feature_row.get("foreign_net_deviation", 0)) > 3:
            return "FOREIGN_MASSIVE_TRADE"
        
        # ê¸°ê´€ ì´ìƒ ê±°ë˜
        elif abs(feature_row.get("institution_net_change", 0)) > 5:
            return "INSTITUTION_UNUSUAL_ACTIVITY"
        
        # ê±°ë˜ëŸ‰ ê¸‰ì¦
        elif feature_row.get("volume_deviation", 0) > 3:
            return "VOLUME_SPIKE"
        
        # ìŠ¤ë§ˆíŠ¸ë¨¸ë‹ˆ ë™ì¡°í™”
        elif feature_row.get("foreign_institution_alignment", 0) == 1 and \
             abs(feature_row.get("smart_money_flow", 0)) > 2:
            return "SMART_MONEY_COORDINATION"
        
        # ì‹œê°„ëŒ€ ì´ìƒ ê±°ë˜
        elif feature_row.get("is_closing", 0) == 1 and \
             abs(feature_row.get("foreign_net_ratio", 0)) > 0.8:
            return "END_OF_DAY_MANIPULATION"
        
        else:
            return "UNKNOWN_PATTERN"
    
    def _calculate_anomaly_severity(self, anomaly_score: float) -> float:
        """ì´ìƒ ê±°ë˜ ì‹¬ê°ë„ ê³„ì‚° (0-10 ìŠ¤ì¼€ì¼)"""
        
        # Isolation Forestì˜ ì ìˆ˜ëŠ” ìŒìˆ˜ (ë” ìŒìˆ˜ì¼ìˆ˜ë¡ ì´ìƒ)
        normalized_score = abs(anomaly_score)
        
        # 0-10 ìŠ¤ì¼€ì¼ë¡œ ë³€í™˜
        severity = min(10.0, max(0.0, normalized_score * 20))
        
        return round(severity, 1)
    
    async def detect_pattern_clusters(self, trading_data: pd.DataFrame) -> List[Dict]:
        """íŒ¨í„´ í´ëŸ¬ìŠ¤í„° ê°ì§€"""
        
        if trading_data.empty or len(trading_data) < 10:
            return []
        
        # íŠ¹ì§• ì¶”ì¶œ
        features = self._extract_pattern_features(trading_data)
        
        if features.empty:
            return []
        
        # ì •ê·œí™”
        features_scaled = self.scalers["trading"].fit_transform(features)
        
        # í´ëŸ¬ìŠ¤í„°ë§
        cluster_labels = self.models["clustering"].fit_predict(features_scaled)
        
        # í´ëŸ¬ìŠ¤í„° ë¶„ì„
        clusters = []
        unique_labels = set(cluster_labels)
        
        for label in unique_labels:
            if label == -1:  # ë…¸ì´ì¦ˆ ì œì™¸
                continue
            
            cluster_indices = np.where(cluster_labels == label)[0]
            cluster_data = trading_data.iloc[cluster_indices]
            
            cluster_analysis = self._analyze_cluster_pattern(cluster_data, features.iloc[cluster_indices])
            
            if cluster_analysis["significance"] >= 7.0:
                clusters.append(cluster_analysis)
        
        return sorted(clusters, key=lambda x: x["significance"], reverse=True)
```

### Day 13-14: ìºì‹± ì‹œìŠ¤í…œ ë° ì„±ëŠ¥ ìµœì í™”

#### 13.1 ê³ ì„±ëŠ¥ ìºì‹± ì‹œìŠ¤í…œ
**src/utils/advanced_cache.py**
```python
import asyncio
import pickle
import zlib
from typing import Any, Dict, List, Optional, Callable
from datetime import datetime, timedelta
import aioredis
from dataclasses import dataclass
import hashlib

@dataclass
class CacheConfig:
    ttl: int
    compress: bool = False
    serialize_method: str = "pickle"  # pickle, json
    invalidation_strategy: str = "ttl"  # ttl, lru, dependency

class AdvancedCacheManager:
    def __init__(self, redis_url: str, default_ttl: int = 300):
        self.redis_url = redis_url
        self.redis_client = None
        self.default_ttl = default_ttl
        self.memory_cache = {}
        self.cache_stats = {
            "hits": 0,
            "misses": 0,
            "memory_hits": 0,
            "redis_hits": 0
        }
        self.dependency_graph = {}  # ìºì‹œ ì˜ì¡´ì„± ê·¸ë˜í”„
        
    async def initialize(self):
        """ìºì‹œ ê´€ë¦¬ì ì´ˆê¸°í™”"""
        self.redis_client = await aioredis.from_url(self.redis_url)
        
        # ì •ë¦¬ íƒœìŠ¤í¬ ì‹œì‘
        asyncio.create_task(self._cleanup_task())
        asyncio.create_task(self._stats_reporting_task())
    
    async def get(
        self,
        key: str,
        fetch_func: Optional[Callable] = None,
        config: Optional[CacheConfig] = None
    ) -> Any:
        """ë‹¤ì¸µ ìºì‹œì—ì„œ ë°ì´í„° ì¡°íšŒ"""
        
        start_time = datetime.now()
        
        try:
            # 1. ë©”ëª¨ë¦¬ ìºì‹œ í™•ì¸
            memory_result = self._get_from_memory(key)
            if memory_result is not None:
                self.cache_stats["hits"] += 1
                self.cache_stats["memory_hits"] += 1
                return memory_result
            
            # 2. Redis ìºì‹œ í™•ì¸
            redis_result = await self._get_from_redis(key, config)
            if redis_result is not None:
                # ë©”ëª¨ë¦¬ ìºì‹œì—ë„ ì €ì¥
                self._save_to_memory(key, redis_result, config)
                self.cache_stats["hits"] += 1
                self.cache_stats["redis_hits"] += 1
                return redis_result
            
            # 3. ìºì‹œ ë¯¸ìŠ¤ - ë°ì´í„° fetch
            if fetch_func:
                self.cache_stats["misses"] += 1
                data = await fetch_func() if asyncio.iscoroutinefunction(fetch_func) else fetch_func()
                
                # ìºì‹œì— ì €ì¥
                await self.set(key, data, config)
                return data
            
            return None
            
        finally:
            # ì„±ëŠ¥ ë©”íŠ¸ë¦­ ê¸°ë¡
            duration = (datetime.now() - start_time).total_seconds()
            await self._record_performance_metric(key, duration)
    
    async def set(
        self,
        key: str,
        value: Any,
        config: Optional[CacheConfig] = None
    ):
        """ìºì‹œì— ë°ì´í„° ì €ì¥"""
        
        if config is None:
            config = CacheConfig(ttl=self.default_ttl)
        
        # ë©”ëª¨ë¦¬ ìºì‹œì— ì €ì¥
        self._save_to_memory(key, value, config)
        
        # Redisì— ì €ì¥
        await self._save_to_redis(key, value, config)
        
        # ì˜ì¡´ì„± ì—…ë°ì´íŠ¸
        await self._update_dependencies(key, value)
    
    async def invalidate(self, pattern: str = None, keys: List[str] = None):
        """ìºì‹œ ë¬´íš¨í™”"""
        
        if keys:
            # íŠ¹ì • í‚¤ë“¤ ë¬´íš¨í™”
            for key in keys:
                await self._invalidate_key(key)
        elif pattern:
            # íŒ¨í„´ ë§¤ì¹­ìœ¼ë¡œ ë¬´íš¨í™”
            await self._invalidate_pattern(pattern)
    
    async def invalidate_dependencies(self, changed_key: str):
        """ì˜ì¡´ì„± ê¸°ë°˜ ìºì‹œ ë¬´íš¨í™”"""
        
        dependent_keys = self.dependency_graph.get(changed_key, [])
        
        for dep_key in dependent_keys:
            await self._invalidate_key(dep_key)
            # ì¬ê·€ì ìœ¼ë¡œ ì˜ì¡´ì„± ë¬´íš¨í™”
            await self.invalidate_dependencies(dep_key)
    
    def _get_from_memory(self, key: str) -> Any:
        """ë©”ëª¨ë¦¬ ìºì‹œì—ì„œ ì¡°íšŒ"""
        
        if key in self.memory_cache:
            entry = self.memory_cache[key]
            
            # TTL í™•ì¸
            if entry["expires"] > datetime.now():
                return entry["data"]
            else:
                # ë§Œë£Œëœ ë°ì´í„° ì œê±°
                del self.memory_cache[key]
        
        return None
    
    def _save_to_memory(self, key: str, value: Any, config: CacheConfig):
        """ë©”ëª¨ë¦¬ ìºì‹œì— ì €ì¥"""
        
        expires = datetime.now() + timedelta(seconds=config.ttl)
        
        self.memory_cache[key] = {
            "data": value,
            "expires": expires,
            "created": datetime.now()
        }
        
        # ë©”ëª¨ë¦¬ ìºì‹œ í¬ê¸° ê´€ë¦¬
        if len(self.memory_cache) > 10000:  # ìµœëŒ€ 10,000ê°œ í•­ëª©
            self._evict_memory_cache()
    
    async def _get_from_redis(self, key: str, config: Optional[CacheConfig]) -> Any:
        """Redisì—ì„œ ì¡°íšŒ"""
        
        try:
            raw_data = await self.redis_client.get(key)
            if raw_data:
                return self._deserialize(raw_data, config)
        except Exception as e:
            self.logger.error(f"Redis get error for key {key}: {e}")
        
        return None
    
    async def _save_to_redis(self, key: str, value: Any, config: CacheConfig):
        """Redisì— ì €ì¥"""
        
        try:
            serialized = self._serialize(value, config)
            await self.redis_client.setex(key, config.ttl, serialized)
        except Exception as e:
            self.logger.error(f"Redis set error for key {key}: {e}")
    
    def _serialize(self, value: Any, config: CacheConfig) -> bytes:
        """ë°ì´í„° ì§ë ¬í™”"""
        
        if config.serialize_method == "pickle":
            data = pickle.dumps(value)
        else:  # JSON
            import json
            data = json.dumps(value).encode()
        
        if config.compress:
            data = zlib.compress(data)
        
        return data
    
    def _deserialize(self, data: bytes, config: Optional[CacheConfig]) -> Any:
        """ë°ì´í„° ì—­ì§ë ¬í™”"""
        
        if config and config.compress:
            data = zlib.decompress(data)
        
        if config and config.serialize_method == "json":
            import json
            return json.loads(data.decode())
        else:  # pickle (ê¸°ë³¸ê°’)
            return pickle.loads(data)
    
    async def _cleanup_task(self):
        """ì£¼ê¸°ì  ìºì‹œ ì •ë¦¬"""
        
        while True:
            try:
                # ë©”ëª¨ë¦¬ ìºì‹œ ì •ë¦¬
                self._cleanup_memory_cache()
                
                # Redis ë©”íŠ¸ë¦­ ì •ë¦¬
                await self._cleanup_redis_metrics()
                
                await asyncio.sleep(300)  # 5ë¶„ë§ˆë‹¤ ì •ë¦¬
                
            except Exception as e:
                self.logger.error(f"Cache cleanup error: {e}")
                await asyncio.sleep(60)
    
    def _cleanup_memory_cache(self):
        """ë§Œë£Œëœ ë©”ëª¨ë¦¬ ìºì‹œ ì •ë¦¬"""
        
        current_time = datetime.now()
        expired_keys = [
            key for key, entry in self.memory_cache.items()
            if entry["expires"] < current_time
        ]
        
        for key in expired_keys:
            del self.memory_cache[key]
    
    async def get_cache_statistics(self) -> Dict:
        """ìºì‹œ í†µê³„ ì¡°íšŒ"""
        
        total_requests = self.cache_stats["hits"] + self.cache_stats["misses"]
        hit_rate = (self.cache_stats["hits"] / total_requests * 100) if total_requests > 0 else 0
        
        # Redis ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰
        redis_info = await self.redis_client.info("memory")
        
        return {
            "hit_rate": round(hit_rate, 2),
            "total_requests": total_requests,
            "memory_cache_size": len(self.memory_cache),
            "redis_memory_usage": redis_info.get("used_memory_human", "N/A"),
            "cache_distribution": {
                "memory_hits": self.cache_stats["memory_hits"],
                "redis_hits": self.cache_stats["redis_hits"],
                "misses": self.cache_stats["misses"]
            }
        }
```

#### 13.2 ì¿¼ë¦¬ ìµœì í™” ë° ì¸ë±ì‹±
**database/optimization.sql**
```sql
-- ì„±ëŠ¥ ìµœì í™”ë¥¼ ìœ„í•œ ì¶”ê°€ ì¸ë±ìŠ¤

-- 1. íˆ¬ììë³„ ì‹œê³„ì—´ ì¡°íšŒ ìµœì í™”
CREATE INDEX CONCURRENTLY idx_investor_trading_foreign_time 
ON investor_trading (foreign_net, timestamp DESC) 
WHERE foreign_net != 0;

CREATE INDEX CONCURRENTLY idx_investor_trading_institution_time 
ON investor_trading (institution_net, timestamp DESC) 
WHERE institution_net != 0;

-- 2. ì¢…ëª©ë³„ íˆ¬ìì ë™í–¥ ìµœì í™”
CREATE INDEX CONCURRENTLY idx_investor_trading_stock_investor 
ON investor_trading (stock_code, timestamp DESC, foreign_net, institution_net);

-- 3. ëŒ€ìš©ëŸ‰ ì§‘ê³„ ì¿¼ë¦¬ ìµœì í™”
CREATE INDEX CONCURRENTLY idx_investor_trading_market_time_partial 
ON investor_trading (market, timestamp DESC) 
WHERE stock_code IS NULL;

-- 4. ìŠ¤ë§ˆíŠ¸ë¨¸ë‹ˆ ì‹ í˜¸ ì¡°íšŒ ìµœì í™”
CREATE INDEX CONCURRENTLY idx_smart_money_composite 
ON smart_money_signals (confidence DESC, timestamp DESC, stock_code);

-- 5. ê¸°ê´€ íˆ¬ìì ì„¸ë¶€ ë¶„ì„ìš© ì¸ë±ìŠ¤
CREATE INDEX CONCURRENTLY idx_institution_detail_composite 
ON institution_detail (stock_code, institution_type, timestamp DESC);

-- 6. ì‹¤ì‹œê°„ ì¡°íšŒìš© ë¶€ë¶„ ì¸ë±ìŠ¤
CREATE INDEX CONCURRENTLY idx_investor_trading_recent 
ON investor_trading (timestamp DESC, stock_code) 
WHERE timestamp >= NOW() - INTERVAL '24 hours';

-- 7. í”„ë¡œê·¸ë¨ ë§¤ë§¤ ë¶„ì„ìš© ì¸ë±ìŠ¤
CREATE INDEX CONCURRENTLY idx_program_trading_time_type 
ON program_trading_detail (timestamp DESC, program_type, stock_code);

-- ì§‘ê³„ ì„±ëŠ¥ì„ ìœ„í•œ materialized view
CREATE MATERIALIZED VIEW mv_daily_investor_summary AS
SELECT 
    DATE(timestamp) as trade_date,
    stock_code,
    market,
    SUM(foreign_buy) as daily_foreign_buy,
    SUM(foreign_sell) as daily_foreign_sell,
    SUM(foreign_net) as daily_foreign_net,
    SUM(institution_buy) as daily_institution_buy,
    SUM(institution_sell) as daily_institution_sell,
    SUM(institution_net) as daily_institution_net,
    COUNT(*) as record_count
FROM investor_trading
WHERE timestamp >= CURRENT_DATE - INTERVAL '90 days'
GROUP BY DATE(timestamp), stock_code, market;

-- materialized view ì¸ë±ìŠ¤
CREATE UNIQUE INDEX idx_mv_daily_summary_primary 
ON mv_daily_investor_summary (trade_date, COALESCE(stock_code, ''), market);

-- ìë™ ê°±ì‹ ì„ ìœ„í•œ í•¨ìˆ˜
CREATE OR REPLACE FUNCTION refresh_daily_summary()
RETURNS void AS $$
BEGIN
    REFRESH MATERIALIZED VIEW CONCURRENTLY mv_daily_investor_summary;
END;
$$ LANGUAGE plpgsql;

-- ì¼ë³„ ìë™ ê°±ì‹  (cron job í•„ìš”)
-- 0 1 * * * psql -d investor_trends -c "SELECT refresh_daily_summary();"

-- ì¿¼ë¦¬ ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§ ë·°
CREATE VIEW v_slow_queries AS
SELECT 
    query,
    calls,
    total_time,
    mean_time,
    rows,
    100.0 * shared_blks_hit / nullif(shared_blks_hit + shared_blks_read, 0) AS hit_percent
FROM pg_stat_statements 
WHERE mean_time > 100  -- 100ms ì´ìƒ ì¿¼ë¦¬
ORDER BY mean_time DESC;

-- ì¸ë±ìŠ¤ ì‚¬ìš©ë¥  ëª¨ë‹ˆí„°ë§
CREATE VIEW v_index_usage AS
SELECT 
    schemaname,
    tablename,
    indexname,
    idx_tup_read,
    idx_tup_fetch,
    idx_scan,
    CASE 
        WHEN idx_scan = 0 THEN 'Unused'
        WHEN idx_scan < 10 THEN 'Low Usage'
        ELSE 'Active'
    END as usage_status
FROM pg_stat_user_indexes
ORDER BY idx_scan DESC;
```

### Day 15: í†µí•© í…ŒìŠ¤íŠ¸ ë° ë°°í¬ ì¤€ë¹„

#### 15.1 í†µí•© í…ŒìŠ¤íŠ¸ ìŠ¤ìœ„íŠ¸
**tests/integration/test_full_workflow.py**
```python
import pytest
import asyncio
from datetime import datetime, timedelta
import pandas as pd

from src.server import InvestorTrendsMCPServer
from src.utils.database import DatabaseManager
from src.utils.advanced_cache import AdvancedCacheManager

class TestFullWorkflow:
    @pytest.fixture(scope="class")
    async def server(self):
        """í…ŒìŠ¤íŠ¸ìš© ì„œë²„ ì¸ìŠ¤í„´ìŠ¤"""
        server = InvestorTrendsMCPServer()
        await server.initialize()
        yield server
        await server.cleanup()
    
    @pytest.mark.asyncio
    async def test_complete_investor_analysis_workflow(self, server):
        """ì™„ì „í•œ íˆ¬ìì ë¶„ì„ ì›Œí¬í”Œë¡œìš° í…ŒìŠ¤íŠ¸"""
        
        # 1. ê¸°ë³¸ íˆ¬ìì ë™í–¥ ì¡°íšŒ
        basic_result = await server.get_investor_trading(
            investor_type="ALL",
            period="1D",
            market="KOSPI"
        )
        
        assert "investor_data" in basic_result
        assert "foreign" in basic_result["investor_data"]
        assert "institution" in basic_result["investor_data"]
        assert "individual" in basic_result["investor_data"]
        
        # 2. íŠ¹ì • ì¢…ëª© ë¶„ì„
        stock_result = await server.get_investor_trading(
            stock_code="005930",  # ì‚¼ì„±ì „ì
            investor_type="FOREIGN",
            period="5D",
            include_details=True
        )
        
        assert stock_result["scope"] == "STOCK"
        assert "stock_info" in stock_result
        assert stock_result["stock_info"]["code"] == "005930"
        
        # 3. í”„ë¡œê·¸ë¨ ë§¤ë§¤ ë¶„ì„
        program_result = await server.get_program_trading(
            market="KOSPI",
            program_type="ALL"
        )
        
        assert "program_trading" in program_result
        assert "summary" in program_result["program_trading"]
        assert "arbitrage" in program_result["program_trading"]
        
        # 4. ìŠ¤ë§ˆíŠ¸ë¨¸ë‹ˆ ì¶”ì 
        smart_money_result = await server.get_smart_money_tracker(
            detection_method="LARGE_ORDERS",
            min_confidence=7.0
        )
        
        assert "smart_money_signals" in smart_money_result
        assert "market_smart_money_index" in smart_money_result
        
        # 5. ë°ì´í„° ì¼ê´€ì„± ê²€ì¦
        await self._verify_data_consistency(basic_result, stock_result, program_result)
    
    async def _verify_data_consistency(self, basic_result, stock_result, program_result):
        """ë°ì´í„° ì¼ê´€ì„± ê²€ì¦"""
        
        # íˆ¬ììë³„ ìˆœë§¤ìˆ˜ í•©ê³„ëŠ” 0ì— ê°€ê¹Œì›Œì•¼ í•¨ (ì‹œì¥ ì „ì²´)
        if basic_result["scope"] == "MARKET":
            foreign_net = basic_result["investor_data"]["foreign"]["net_amount"]
            institution_net = basic_result["investor_data"]["institution"]["net_amount"]
            individual_net = basic_result["investor_data"]["individual"]["net_amount"]
            
            total_net = foreign_net + institution_net + individual_net
            # ì˜¤ì°¨ í—ˆìš© (1ì–µì›)
            assert abs(total_net) < 100000000, f"Market balance inconsistent: {total_net}"
        
        # í”„ë¡œê·¸ë¨ ë§¤ë§¤ ë°ì´í„° ê²€ì¦
        program_summary = program_result["program_trading"]["summary"]
        assert program_summary["total_buy"] >= 0
        assert program_summary["total_sell"] >= 0
        assert 0 <= program_summary["buy_ratio"] <= 100
    
    @pytest.mark.asyncio
    async def test_performance_under_load(self, server):
        """ë¶€í•˜ ìƒí™©ì—ì„œì˜ ì„±ëŠ¥ í…ŒìŠ¤íŠ¸"""
        
        # ë™ì‹œ ìš”ì²­ ì‹œë®¬ë ˆì´ì…˜
        tasks = []
        
        for i in range(50):  # 50ê°œ ë™ì‹œ ìš”ì²­
            if i % 3 == 0:
                task = server.get_investor_trading()
            elif i % 3 == 1:
                task = server.get_program_trading()
            else:
                task = server.get_smart_money_tracker()
            
            tasks.append(task)
        
        start_time = datetime.now()
        results = await asyncio.gather(*tasks, return_exceptions=True)
        end_time = datetime.now()
        
        # ì„±ëŠ¥ ê²€ì¦
        duration = (end_time - start_time).total_seconds()
        assert duration < 30, f"Performance too slow: {duration}s for 50 requests"
        
        # ì—ëŸ¬ìœ¨ ê²€ì¦
        errors = [r for r in results if isinstance(r, Exception)]
        error_rate = len(errors) / len(results)
        assert error_rate < 0.1, f"Error rate too high: {error_rate * 100}%"
    
    @pytest.mark.asyncio
    async def test_cache_effectiveness(self, server):
        """ìºì‹œ íš¨ê³¼ì„± í…ŒìŠ¤íŠ¸"""
        
        # ì²« ë²ˆì§¸ ìš”ì²­ (ìºì‹œ ë¯¸ìŠ¤)
        start_time = datetime.now()
        result1 = await server.get_investor_trading(stock_code="005930")
        first_duration = (datetime.now() - start_time).total_seconds()
        
        # ë‘ ë²ˆì§¸ ìš”ì²­ (ìºì‹œ íˆíŠ¸)
        start_time = datetime.now()
        result2 = await server.get_investor_trading(stock_code="005930")
        second_duration = (datetime.now() - start_time).total_seconds()
        
        # ìºì‹œ íš¨ê³¼ ê²€ì¦
        assert second_duration < first_duration * 0.5, "Cache not effective"
        assert result1 == result2, "Cached result inconsistent"
        
        # ìºì‹œ í†µê³„ í™•ì¸
        cache_stats = await server.cache_manager.get_cache_statistics()
        assert cache_stats["hit_rate"] > 0, "No cache hits recorded"
    
    @pytest.mark.asyncio
    async def test_database_performance(self, server):
        """ë°ì´í„°ë² ì´ìŠ¤ ì„±ëŠ¥ í…ŒìŠ¤íŠ¸"""
        
        db_manager = server.db_manager
        
        # ëŒ€ëŸ‰ ë°ì´í„° ì‚½ì… í…ŒìŠ¤íŠ¸
        test_data = []
        for i in range(1000):
            test_data.append({
                "timestamp": datetime.now() - timedelta(minutes=i),
                "stock_code": f"00{i % 100:04d}",
                "market": "KOSPI",
                "foreign_buy": i * 1000000,
                "foreign_sell": i * 900000,
                "foreign_net": i * 100000,
                "institution_buy": i * 800000,
                "institution_sell": i * 850000,
                "institution_net": -i * 50000,
                "individual_buy": i * 700000,
                "individual_sell": i * 750000,
                "individual_net": -i * 50000,
                "program_buy": i * 200000,
                "program_sell": i * 180000,
                "program_net": i * 20000
            })
        
        start_time = datetime.now()
        await db_manager.batch_insert_investor_trading(test_data)
        insert_duration = (datetime.now() - start_time).total_seconds()
        
        # ì„±ëŠ¥ ê¸°ì¤€: 1000ê±´ ì‚½ì…ì´ 5ì´ˆ ì´ë‚´
        assert insert_duration < 5.0, f"Insert too slow: {insert_duration}s"
        
        # ì¡°íšŒ ì„±ëŠ¥ í…ŒìŠ¤íŠ¸
        start_time = datetime.now()
        result = await db_manager.get_investor_trading_history(
            market="KOSPI",
            hours=24
        )
        query_duration = (datetime.now() - start_time).total_seconds()
        
        # ì„±ëŠ¥ ê¸°ì¤€: 24ì‹œê°„ ë°ì´í„° ì¡°íšŒê°€ 2ì´ˆ ì´ë‚´
        assert query_duration < 2.0, f"Query too slow: {query_duration}s"
        assert len(result) > 0, "No data returned"
    
    @pytest.mark.asyncio
    async def test_error_handling_and_recovery(self, server):
        """ì—ëŸ¬ ì²˜ë¦¬ ë° ë³µêµ¬ í…ŒìŠ¤íŠ¸"""
        
        # ì˜ëª»ëœ íŒŒë¼ë¯¸í„° í…ŒìŠ¤íŠ¸
        with pytest.raises(ValueError):
            await server.get_investor_trading(
                stock_code="INVALID",
                investor_type="UNKNOWN"
            )
        
        # API íƒ€ì„ì•„ì›ƒ ì‹œë®¬ë ˆì´ì…˜
        # (ì‹¤ì œë¡œëŠ” mockì„ ì‚¬ìš©í•˜ì—¬ ì‹œë®¬ë ˆì´ì…˜)
        
        # ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²° ì‹¤íŒ¨ ì‹œë®¬ë ˆì´ì…˜
        # (ì‹¤ì œë¡œëŠ” mockì„ ì‚¬ìš©í•˜ì—¬ ì‹œë®¬ë ˆì´ì…˜)
        
        # ì„œë²„ê°€ ì •ìƒì ìœ¼ë¡œ ë³µêµ¬ë˜ëŠ”ì§€ í™•ì¸
        recovery_result = await server.get_investor_trading()
        assert recovery_result is not None, "Server failed to recover"
```

#### 15.2 ë°°í¬ ìŠ¤í¬ë¦½íŠ¸
**deploy/deploy.sh**
```bash
#!/bin/bash

set -e

echo "ğŸš€ Starting MCP Investor Trends Server Deployment"

# í™˜ê²½ ë³€ìˆ˜ í™•ì¸
if [ -z "$DEPLOYMENT_ENV" ]; then
    echo "âŒ DEPLOYMENT_ENV not set (dev/staging/prod)"
    exit 1
fi

# ì„¤ì • íŒŒì¼ ë¡œë“œ
source "config/${DEPLOYMENT_ENV}.env"

echo "ğŸ“ Deploying to environment: $DEPLOYMENT_ENV"

# 1. ì˜ì¡´ì„± í™•ì¸
echo "ğŸ” Checking dependencies..."
python --version
pip --version
docker --version
docker-compose --version

# 2. ì½”ë“œ í’ˆì§ˆ ê²€ì‚¬
echo "ğŸ§¹ Running code quality checks..."
python -m flake8 src/ --max-line-length=100
python -m mypy src/ --ignore-missing-imports
python -m black src/ --check

# 3. í…ŒìŠ¤íŠ¸ ì‹¤í–‰
echo "ğŸ§ª Running tests..."
python -m pytest tests/ -v --cov=src --cov-report=html

# 4. ë³´ì•ˆ ê²€ì‚¬
echo "ğŸ”’ Running security checks..."
python -m bandit -r src/ -f json -o security-report.json

# 5. ë°ì´í„°ë² ì´ìŠ¤ ë§ˆì´ê·¸ë ˆì´ì…˜ (staging/prodë§Œ)
if [ "$DEPLOYMENT_ENV" != "dev" ]; then
    echo "ğŸ—„ï¸ Running database migrations..."
    python scripts/migrate_database.py --env=$DEPLOYMENT_ENV
fi

# 6. Docker ì´ë¯¸ì§€ ë¹Œë“œ
echo "ğŸ—ï¸ Building Docker image..."
docker build -t mcp-investor-trends:$DEPLOYMENT_ENV .

# 7. ì´ë¯¸ì§€ ìµœì í™” í™•ì¸
echo "ğŸ“Š Checking image size..."
docker images mcp-investor-trends:$DEPLOYMENT_ENV --format "table {{.Repository}}\t{{.Tag}}\t{{.Size}}"

# 8. í™˜ê²½ë³„ ë°°í¬
if [ "$DEPLOYMENT_ENV" = "dev" ]; then
    echo "ğŸƒâ€â™‚ï¸ Starting development environment..."
    docker-compose -f docker-compose.dev.yml up -d
    
elif [ "$DEPLOYMENT_ENV" = "staging" ]; then
    echo "ğŸ­ Deploying to staging..."
    docker-compose -f docker-compose.staging.yml up -d
    
    # í—¬ìŠ¤ ì²´í¬
    echo "â¤ï¸ Running health checks..."
    sleep 30
    python scripts/health_check.py --env=staging
    
elif [ "$DEPLOYMENT_ENV" = "prod" ]; then
    echo "ğŸš€ Deploying to production..."
    
    # ë¸”ë£¨-ê·¸ë¦° ë°°í¬
    python scripts/blue_green_deploy.py
    
    # íŠ¸ë˜í”½ ì „í™˜ ì „ ìµœì¢… ê²€ì¦
    echo "âœ… Running production validation..."
    python scripts/production_validation.py
    
    # ëª¨ë‹ˆí„°ë§ ì•Œë¦¼ í™œì„±í™”
    python scripts/enable_monitoring.py
fi

# 9. ë°°í¬ í›„ ê²€ì¦
echo "ğŸ” Post-deployment verification..."
sleep 10

# API ì—”ë“œí¬ì¸íŠ¸ í…ŒìŠ¤íŠ¸
curl -f http://localhost:8080/health || {
    echo "âŒ Health check failed"
    exit 1
}

# MCP ì„œë²„ ì—°ê²° í…ŒìŠ¤íŠ¸
python scripts/test_mcp_connection.py

echo "âœ… Deployment completed successfully!"

# 10. ìŠ¬ë™ ì•Œë¦¼ (ì„ íƒì‚¬í•­)
if [ ! -z "$SLACK_WEBHOOK_URL" ]; then
    curl -X POST -H 'Content-type: application/json' \
        --data "{\"text\":\"âœ… MCP Investor Trends deployed to $DEPLOYMENT_ENV successfully\"}" \
        $SLACK_WEBHOOK_URL
fi

echo "ğŸ“Š Deployment summary:"
echo "Environment: $DEPLOYMENT_ENV"
echo "Version: $(git rev-parse --short HEAD)"
echo "Timestamp: $(date)"
echo "Image: mcp-investor-trends:$DEPLOYMENT_ENV"
```

ì´ì œ ìƒì„¸í•œ ê°œë°œê³„íšì´ ì™„ì„±ë˜ì—ˆìŠµë‹ˆë‹¤. ë‹¤ìŒ ë‹¨ê³„ë¡œ ì„¸ë¶€ ì‘ì—… í•­ëª©ê³¼ ì¼ì •ì„ ìˆ˜ë¦½í•˜ê² ìŠµë‹ˆë‹¤.

<function_calls>
<invoke name="TodoWrite">
<parameter name="todos">[{"id": "1", "content": "ê¸°ì¡´ ê³„íšë¬¸ì„œ ë¶„ì„ ë° ê°œë°œ ë²”ìœ„ ì •ì˜", "status": "completed", "priority": "high"}, {"id": "2", "content": "ë‹¨ê³„ë³„ êµ¬ì²´ì  ê°œë°œê³„íš ì‘ì„±", "status": "completed", "priority": "high"}, {"id": "3", "content": "ê° ë‹¨ê³„ë³„ ì„¸ë¶€ ì‘ì—… í•­ëª© ë° ì¼ì • ìˆ˜ë¦½", "status": "in_progress", "priority": "high"}, {"id": "4", "content": "ê¸°ìˆ ì  ì˜ì¡´ì„± ë° ìœ„í—˜ìš”ì†Œ ë¶„ì„", "status": "pending", "priority": "medium"}]